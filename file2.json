[{"layout_dets": [{"category_id": 1, "poly": [658.3194580078125, 790.7893676757812, 1044.8739013671875, 790.7893676757812, 1044.8739013671875, 856.0017700195312, 658.3194580078125, 856.0017700195312], "score": 0.9999971389770508}, {"category_id": 1, "poly": [392.2400817871094, 996.23291015625, 1307.1201171875, 996.23291015625, 1307.1201171875, 1394.957763671875, 392.2400817871094, 1394.957763671875], "score": 0.9999842643737793}, {"category_id": 1, "poly": [295.8812255859375, 1507.8092041015625, 1406.173828125, 1507.8092041015625, 1406.173828125, 1666.696533203125, 295.8812255859375, 1666.696533203125], "score": 0.9999480247497559}, {"category_id": 0, "poly": [299.5628662109375, 1440.188720703125, 536.3953247070312, 1440.188720703125, 536.3953247070312, 1480.1763916015625, 299.5628662109375, 1480.1763916015625], "score": 0.9998817443847656}, {"category_id": 0, "poly": [788.5079345703125, 932.9938354492188, 913.7645263671875, 932.9938354492188, 913.7645263671875, 965.5443725585938, 788.5079345703125, 965.5443725585938], "score": 0.9997336268424988}, {"category_id": 2, "poly": [295.585205078125, 2032.3927001953125, 1279.83154296875, 2032.3927001953125, 1279.83154296875, 2063.59375, 295.585205078125, 2063.59375], "score": 0.9987990260124207}, {"category_id": 0, "poly": [586.2052001953125, 271.81585693359375, 1111.0042724609375, 271.81585693359375, 1111.0042724609375, 324.487060546875, 586.2052001953125, 324.487060546875], "score": 0.9987422823905945}, {"category_id": 3, "poly": [312.0315856933594, 501.04315185546875, 1390.09521484375, 501.04315185546875, 1390.09521484375, 752.3656005859375, 312.0315856933594, 752.3656005859375], "score": 0.9897851943969727}, {"category_id": 2, "poly": [294.3865661621094, 1674.5859375, 1408.2921142578125, 1674.5859375, 1408.2921142578125, 1980.53173828125, 294.3865661621094, 1980.53173828125], "score": 0.9291009902954102}, {"category_id": 1, "poly": [310.24169921875, 503.35308837890625, 1390.796142578125, 503.35308837890625, 1390.796142578125, 751.745849609375, 310.24169921875, 751.745849609375], "score": 0.48287293314933777}, {"category_id": 1, "poly": [294.1542663574219, 1675.017822265625, 1408.1500244140625, 1675.017822265625, 1408.1500244140625, 1981.9822998046875, 294.1542663574219, 1981.9822998046875], "score": 0.28954941034317017}, {"category_id": 15, "poly": [737.0, 791.0, 972.0, 786.0, 973.0, 825.0, 737.0, 830.0], "score": 0.97, "text": "llia Polosukhin* "}, {"category_id": 15, "poly": [658.0, 823.0, 1046.0, 823.0, 1046.0, 862.0, 658.0, 862.0], "score": 1.0, "text": "illia.polosukhin@gmail.com"}, {"category_id": 15, "poly": [395.0, 999.0, 1305.0, 999.0, 1305.0, 1031.0, 395.0, 1031.0], "score": 1.0, "text": "The dominant sequence transduction models are based on complex recurrent or"}, {"category_id": 15, "poly": [397.0, 1029.0, 1307.0, 1029.0, 1307.0, 1061.0, 397.0, 1061.0], "score": 0.99, "text": "convolutional neural networks that include an encoder and a decoder. The best "}, {"category_id": 15, "poly": [390.0, 1056.0, 1307.0, 1054.0, 1307.0, 1093.0, 390.0, 1095.0], "score": 0.99, "text": " performing models also connect the encoder and decoder through an attention"}, {"category_id": 15, "poly": [395.0, 1086.0, 1307.0, 1086.0, 1307.0, 1125.0, 395.0, 1125.0], "score": 0.99, "text": "mechanism. We propose a new simple network architecture, the Transformer,"}, {"category_id": 15, "poly": [395.0, 1121.0, 1303.0, 1121.0, 1303.0, 1153.0, 395.0, 1153.0], "score": 0.99, "text": "based solely on attention mechanisms, dispensing with recurrence and convolutions"}, {"category_id": 15, "poly": [393.0, 1150.0, 1305.0, 1148.0, 1305.0, 1180.0, 393.0, 1183.0], "score": 1.0, "text": "entirely. Experiments on two machine translation tasks show these models to"}, {"category_id": 15, "poly": [393.0, 1178.0, 1305.0, 1178.0, 1305.0, 1217.0, 393.0, 1217.0], "score": 0.99, "text": "be superior in quality while being more parallelizable and requiring significantly"}, {"category_id": 15, "poly": [395.0, 1210.0, 1303.0, 1210.0, 1303.0, 1240.0, 395.0, 1240.0], "score": 0.98, "text": "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-"}, {"category_id": 15, "poly": [393.0, 1240.0, 1300.0, 1240.0, 1300.0, 1272.0, 393.0, 1272.0], "score": 0.99, "text": "to-German translation task, improving over the existing best results, including"}, {"category_id": 15, "poly": [395.0, 1272.0, 1305.0, 1272.0, 1305.0, 1304.0, 395.0, 1304.0], "score": 0.99, "text": "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,"}, {"category_id": 15, "poly": [395.0, 1302.0, 1303.0, 1302.0, 1303.0, 1334.0, 395.0, 1334.0], "score": 0.99, "text": "our model establishes a new single-model state-of-the-art BLEU score of 41.0 after"}, {"category_id": 15, "poly": [395.0, 1334.0, 1305.0, 1334.0, 1305.0, 1364.0, 395.0, 1364.0], "score": 0.99, "text": "training for 3.5 days on eight GPUs, a small fraction of the training costs of the"}, {"category_id": 15, "poly": [393.0, 1361.0, 748.0, 1364.0, 748.0, 1396.0, 393.0, 1393.0], "score": 1.0, "text": "best models from the literature."}, {"category_id": 15, "poly": [298.0, 1512.0, 1400.0, 1512.0, 1400.0, 1542.0, 298.0, 1542.0], "score": 0.99, "text": "Recurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks"}, {"category_id": 15, "poly": [296.0, 1545.0, 1404.0, 1545.0, 1404.0, 1577.0, 296.0, 1577.0], "score": 0.99, "text": "in particular, have been firmly established as state of the art approaches in sequence modeling and"}, {"category_id": 15, "poly": [296.0, 1574.0, 1402.0, 1574.0, 1402.0, 1606.0, 296.0, 1606.0], "score": 0.99, "text": "transduction problems such as language modeling and machine translation [29, 2, 5]. Numerous"}, {"category_id": 15, "poly": [298.0, 1604.0, 1402.0, 1604.0, 1402.0, 1636.0, 298.0, 1636.0], "score": 0.99, "text": "efforts have since continued to push the boundaries of recurrent language models and encoder-decoder"}, {"category_id": 15, "poly": [296.0, 1634.0, 587.0, 1634.0, 587.0, 1666.0, 296.0, 1666.0], "score": 0.99, "text": "architectures [31, 21, 13]."}, {"category_id": 15, "poly": [296.0, 1446.0, 534.0, 1446.0, 534.0, 1485.0, 296.0, 1485.0], "score": 0.92, "text": "1 \u2014Introduction"}, {"category_id": 15, "poly": [783.0, 933.0, 912.0, 933.0, 912.0, 974.0, 783.0, 974.0], "score": 1.0, "text": "Abstract"}, {"category_id": 15, "poly": [300.0, 2035.0, 1275.0, 2035.0, 1275.0, 2067.0, 300.0, 2067.0], "score": 0.99, "text": "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA."}, {"category_id": 15, "poly": [591.0, 282.0, 1111.0, 282.0, 1111.0, 321.0, 591.0, 321.0], "score": 0.98, "text": "Attention Is All You Need"}, {"category_id": 15, "poly": [333.0, 1675.0, 1402.0, 1678.0, 1402.0, 1710.0, 333.0, 1707.0], "score": 0.99, "text": "*Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started"}, {"category_id": 15, "poly": [296.0, 1705.0, 1402.0, 1705.0, 1402.0, 1737.0, 296.0, 1737.0], "score": 0.99, "text": "the effort to evaluate this idea. Ashish, with Ilia, designed and implemented the first Transformer models and"}, {"category_id": 15, "poly": [296.0, 1735.0, 1404.0, 1735.0, 1404.0, 1767.0, 296.0, 1767.0], "score": 0.99, "text": "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head"}, {"category_id": 15, "poly": [298.0, 1762.0, 1402.0, 1762.0, 1402.0, 1794.0, 298.0, 1794.0], "score": 0.99, "text": "attention and the parameter-free position representation and became the other person involved in nearly every"}, {"category_id": 15, "poly": [298.0, 1790.0, 1402.0, 1790.0, 1402.0, 1822.0, 298.0, 1822.0], "score": 0.99, "text": "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and"}, {"category_id": 15, "poly": [298.0, 1817.0, 1404.0, 1817.0, 1404.0, 1849.0, 298.0, 1849.0], "score": 0.99, "text": "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and"}, {"category_id": 15, "poly": [296.0, 1845.0, 1407.0, 1845.0, 1407.0, 1877.0, 296.0, 1877.0], "score": 0.99, "text": "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and "}, {"category_id": 15, "poly": [296.0, 1872.0, 1407.0, 1872.0, 1407.0, 1904.0, 296.0, 1904.0], "score": 0.99, "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating "}, {"category_id": 15, "poly": [296.0, 1902.0, 430.0, 1902.0, 430.0, 1927.0, 296.0, 1927.0], "score": 1.0, "text": "ourresearch."}, {"category_id": 15, "poly": [328.0, 1923.0, 749.0, 1927.0, 748.0, 1966.0, 328.0, 1962.0], "score": 0.96, "text": "t Work performed while at Google Brain."}, {"category_id": 15, "poly": [328.0, 1955.0, 779.0, 1959.0, 778.0, 1992.0, 328.0, 1987.0], "score": 0.94, "text": "+Work nerformed while. at Google. Research."}, {"category_id": 15, "poly": [365.0, 513.0, 566.0, 513.0, 566.0, 545.0, 365.0, 545.0], "score": 1.0, "text": "Ashish Vaswani*"}, {"category_id": 15, "poly": [661.0, 513.0, 848.0, 513.0, 848.0, 545.0, 661.0, 545.0], "score": 1.0, "text": "Noam Shazeer*"}, {"category_id": 15, "poly": [938.0, 513.0, 1104.0, 513.0, 1104.0, 545.0, 938.0, 545.0], "score": 1.0, "text": "Niki Parmar*"}, {"category_id": 15, "poly": [1176.0, 513.0, 1381.0, 513.0, 1381.0, 545.0, 1176.0, 545.0], "score": 1.0, "text": "Jakob Uszkoreit*"}, {"category_id": 15, "poly": [381.0, 545.0, 543.0, 545.0, 543.0, 578.0, 381.0, 578.0], "score": 1.0, "text": "Google Brain"}, {"category_id": 15, "poly": [670.0, 545.0, 832.0, 545.0, 832.0, 578.0, 670.0, 578.0], "score": 1.0, "text": "Google Brain"}, {"category_id": 15, "poly": [917.0, 545.0, 1116.0, 545.0, 1116.0, 578.0, 917.0, 578.0], "score": 0.99, "text": "Google Research"}, {"category_id": 15, "poly": [1173.0, 545.0, 1372.0, 545.0, 1372.0, 578.0, 1173.0, 578.0], "score": 1.0, "text": "Google Research"}, {"category_id": 15, "poly": [319.0, 575.0, 608.0, 578.0, 607.0, 610.0, 319.0, 607.0], "score": 1.0, "text": "avaswani@google.com"}, {"category_id": 15, "poly": [635.0, 575.0, 866.0, 578.0, 866.0, 610.0, 635.0, 607.0], "score": 1.0, "text": "noam@google.com"}, {"category_id": 15, "poly": [894.0, 578.0, 1139.0, 578.0, 1139.0, 610.0, 894.0, 610.0], "score": 1.0, "text": "nikip@google.com"}, {"category_id": 15, "poly": [1169.0, 578.0, 1381.0, 578.0, 1381.0, 610.0, 1169.0, 610.0], "score": 1.0, "text": "usz@google.com"}, {"category_id": 15, "poly": [397.0, 653.0, 547.0, 653.0, 547.0, 685.0, 397.0, 685.0], "score": 0.96, "text": "Llion Jones*"}, {"category_id": 15, "poly": [688.0, 653.0, 903.0, 653.0, 903.0, 685.0, 688.0, 685.0], "score": 1.0, "text": "Aidan N. Gomez*"}, {"category_id": 15, "poly": [1090.0, 653.0, 1277.0, 653.0, 1277.0, 685.0, 1090.0, 685.0], "score": 1.0, "text": "Lukasz Kaiser*"}, {"category_id": 15, "poly": [367.0, 683.0, 566.0, 683.0, 566.0, 715.0, 367.0, 715.0], "score": 0.99, "text": "Google Research"}, {"category_id": 15, "poly": [670.0, 678.0, 926.0, 683.0, 926.0, 722.0, 670.0, 717.0], "score": 0.98, "text": " University of Toronto"}, {"category_id": 15, "poly": [1096.0, 678.0, 1264.0, 683.0, 1263.0, 722.0, 1094.0, 717.0], "score": 0.97, "text": " Google Brain"}, {"category_id": 15, "poly": [347.0, 710.0, 594.0, 715.0, 593.0, 754.0, 346.0, 749.0], "score": 0.99, "text": "llion@google.com"}, {"category_id": 15, "poly": [649.0, 713.0, 949.0, 715.0, 949.0, 747.0, 649.0, 745.0], "score": 1.0, "text": "aidan@cs.toronto.edu"}, {"category_id": 15, "poly": [1010.0, 713.0, 1351.0, 717.0, 1351.0, 750.0, 1009.0, 745.0], "score": 1.0, "text": "lukaszkaiser@google.com"}, {"category_id": 15, "poly": [333.0, 1675.0, 1402.0, 1678.0, 1402.0, 1710.0, 333.0, 1707.0], "score": 0.99, "text": "*Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started"}, {"category_id": 15, "poly": [293.0, 1703.0, 1402.0, 1705.0, 1402.0, 1737.0, 293.0, 1735.0], "score": 0.98, "text": " the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and"}, {"category_id": 15, "poly": [296.0, 1735.0, 1404.0, 1735.0, 1404.0, 1767.0, 296.0, 1767.0], "score": 0.99, "text": "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head"}, {"category_id": 15, "poly": [298.0, 1762.0, 1402.0, 1762.0, 1402.0, 1794.0, 298.0, 1794.0], "score": 0.99, "text": "attention and the parameter-free position representation and became the other person involved in nearly every"}, {"category_id": 15, "poly": [298.0, 1790.0, 1402.0, 1790.0, 1402.0, 1822.0, 298.0, 1822.0], "score": 0.99, "text": "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and"}, {"category_id": 15, "poly": [296.0, 1817.0, 1404.0, 1817.0, 1404.0, 1849.0, 296.0, 1849.0], "score": 0.99, "text": "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and"}, {"category_id": 15, "poly": [296.0, 1845.0, 1407.0, 1845.0, 1407.0, 1877.0, 296.0, 1877.0], "score": 0.99, "text": "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and "}, {"category_id": 15, "poly": [296.0, 1872.0, 1407.0, 1872.0, 1407.0, 1904.0, 296.0, 1904.0], "score": 0.99, "text": "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating "}, {"category_id": 15, "poly": [296.0, 1902.0, 430.0, 1902.0, 430.0, 1927.0, 296.0, 1927.0], "score": 1.0, "text": "ourresearch."}, {"category_id": 15, "poly": [328.0, 1923.0, 749.0, 1927.0, 748.0, 1966.0, 328.0, 1962.0], "score": 0.96, "text": "t Work performed while at Google Brain."}, {"category_id": 15, "poly": [326.0, 1952.0, 783.0, 1957.0, 783.0, 1996.0, 326.0, 1991.0], "score": 0.97, "text": "+Work nerformed while at Google Research."}], "page_info": {"page_no": 0, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 1, "poly": [296.1476135253906, 203.09017944335938, 1406.493896484375, 203.09017944335938, 1406.493896484375, 447.7494812011719, 296.1476135253906, 447.7494812011719], "score": 0.9999921321868896}, {"category_id": 1, "poly": [296.76531982421875, 826.7679443359375, 1406.3529052734375, 826.7679443359375, 1406.3529052734375, 1107.0750732421875, 296.76531982421875, 1107.0750732421875], "score": 0.9999814033508301}, {"category_id": 0, "poly": [297.238525390625, 762.5859985351562, 531.4449462890625, 762.5859985351562, 531.4449462890625, 799.3380737304688, 297.238525390625, 799.3380737304688], "score": 0.9999799728393555}, {"category_id": 0, "poly": [295.7197265625, 1524.2078857421875, 634.9398193359375, 1524.2078857421875, 634.9398193359375, 1563.4471435546875, 295.7197265625, 1563.4471435546875], "score": 0.9999561309814453}, {"category_id": 0, "poly": [296.26025390625, 1886.7061767578125, 707.3759765625, 1886.7061767578125, 707.3759765625, 1922.0894775390625, 296.26025390625, 1922.0894775390625], "score": 0.9999532699584961}, {"category_id": 1, "poly": [297.6603698730469, 460.2660827636719, 1406.6898193359375, 460.2660827636719, 1406.6898193359375, 584.3154296875, 297.6603698730469, 584.3154296875], "score": 0.999929666519165}, {"category_id": 1, "poly": [296.9475402832031, 597.4364013671875, 1405.870361328125, 597.4364013671875, 1405.870361328125, 722.3633422851562, 296.9475402832031, 722.3633422851562], "score": 0.9998970031738281}, {"category_id": 1, "poly": [296.8546142578125, 1118.512939453125, 1406.396728515625, 1118.512939453125, 1406.396728515625, 1242.69775390625, 296.8546142578125, 1242.69775390625], "score": 0.9998628497123718}, {"category_id": 1, "poly": [296.7046203613281, 1593.4439697265625, 1406.201904296875, 1593.4439697265625, 1406.201904296875, 1750.022216796875, 296.7046203613281, 1750.022216796875], "score": 0.9997810125350952}, {"category_id": 1, "poly": [297.2752990722656, 1760.34033203125, 1406.696044921875, 1760.34033203125, 1406.696044921875, 1855.056640625, 297.2752990722656, 1855.056640625], "score": 0.9997708797454834}, {"category_id": 1, "poly": [296.661376953125, 1359.89013671875, 1406.4189453125, 1359.89013671875, 1406.4189453125, 1485.7418212890625, 296.661376953125, 1485.7418212890625], "score": 0.9997485876083374}, {"category_id": 1, "poly": [297.0956726074219, 1255.3895263671875, 1405.7972412109375, 1255.3895263671875, 1405.7972412109375, 1348.827880859375, 297.0956726074219, 1348.827880859375], "score": 0.9997427463531494}, {"category_id": 1, "poly": [297.3306884765625, 1943.96826171875, 1407.352294921875, 1943.96826171875, 1407.352294921875, 2009.0960693359375, 297.3306884765625, 2009.0960693359375], "score": 0.9996616244316101}, {"category_id": 2, "poly": [841.5183715820312, 2061.044189453125, 858.3983764648438, 2061.044189453125, 858.3983764648438, 2085.92822265625, 841.5183715820312, 2085.92822265625], "score": 0.9976586699485779}, {"category_id": 13, "poly": [647, 1655, 834, 1655, 834, 1689, 647, 1689], "score": 0.93, "latex": "\\mathbf{z}~=~\\left(z_{1},...,z_{n}\\right)"}, {"category_id": 13, "poly": [1105, 1625, 1234, 1625, 1234, 1658, 1105, 1658], "score": 0.93, "latex": "(x_{1},...,x_{n})"}, {"category_id": 13, "poly": [410, 1686, 540, 1686, 540, 1719, 410, 1719], "score": 0.92, "latex": "(y_{1},...,y_{m})"}, {"category_id": 13, "poly": [861, 265, 918, 265, 918, 296, 861, 296], "score": 0.91, "latex": "h_{t-1}"}, {"category_id": 13, "poly": [891, 1946, 978, 1946, 978, 1974, 891, 1974], "score": 0.9, "latex": "N=6"}, {"category_id": 13, "poly": [365, 265, 393, 265, 393, 294, 365, 294], "score": 0.88, "latex": "h_{t}"}, {"category_id": 13, "poly": [1207, 267, 1220, 267, 1220, 291, 1207, 291], "score": 0.47, "latex": "t"}, {"category_id": 13, "poly": [935, 1661, 954, 1661, 954, 1683, 935, 1683], "score": 0.41, "latex": "\\mathbf{z}"}, {"category_id": 15, "poly": [293.0, 204.0, 1402.0, 204.0, 1402.0, 236.0, 293.0, 236.0], "score": 0.99, "text": " Recurrent models typically factor computation along the symbol positions of the input and output"}, {"category_id": 15, "poly": [298.0, 236.0, 1402.0, 236.0, 1402.0, 268.0, 298.0, 268.0], "score": 1.0, "text": "sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden"}, {"category_id": 15, "poly": [293.0, 293.0, 1404.0, 293.0, 1404.0, 332.0, 293.0, 332.0], "score": 0.97, "text": " sequential nature precludes parallization within training examples, which becomes critical at longer"}, {"category_id": 15, "poly": [298.0, 328.0, 1404.0, 328.0, 1404.0, 360.0, 298.0, 360.0], "score": 1.0, "text": "sequence lengths, as memory constraints limit batching across examples. Recent work has achieved"}, {"category_id": 15, "poly": [298.0, 357.0, 1402.0, 357.0, 1402.0, 390.0, 298.0, 390.0], "score": 0.98, "text": "significant improvements in computational efficiency through factorization tricks [18] and conditional"}, {"category_id": 15, "poly": [296.0, 387.0, 1404.0, 387.0, 1404.0, 419.0, 296.0, 419.0], "score": 0.99, "text": "computation [26], while also improving model performance in case of the latter. The fundamental"}, {"category_id": 15, "poly": [296.0, 417.0, 917.0, 417.0, 917.0, 449.0, 296.0, 449.0], "score": 1.0, "text": "constraint of sequential computation, however, remains."}, {"category_id": 15, "poly": [296.0, 266.0, 364.0, 266.0, 364.0, 298.0, 296.0, 298.0], "score": 0.77, "text": "states"}, {"category_id": 15, "poly": [394.0, 266.0, 860.0, 266.0, 860.0, 298.0, 394.0, 298.0], "score": 0.99, "text": ", as a function of the previous hidden state"}, {"category_id": 15, "poly": [919.0, 266.0, 1206.0, 266.0, 1206.0, 298.0, 919.0, 298.0], "score": 0.98, "text": "and the input for position"}, {"category_id": 15, "poly": [1221.0, 266.0, 1402.0, 266.0, 1402.0, 298.0, 1221.0, 298.0], "score": 0.98, "text": ".This inherently"}, {"category_id": 15, "poly": [296.0, 832.0, 1402.0, 830.0, 1402.0, 862.0, 296.0, 864.0], "score": 0.99, "text": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU"}, {"category_id": 15, "poly": [298.0, 864.0, 1404.0, 864.0, 1404.0, 896.0, 298.0, 896.0], "score": 1.0, "text": "[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building"}, {"category_id": 15, "poly": [298.0, 894.0, 1402.0, 894.0, 1402.0, 926.0, 298.0, 926.0], "score": 0.99, "text": "block, computing hidden representations in parallel for all input and output positions. In these models,"}, {"category_id": 15, "poly": [296.0, 926.0, 1402.0, 926.0, 1402.0, 956.0, 296.0, 956.0], "score": 0.98, "text": "the number of operations required to relate signals from two arbitrary input or output positions grows"}, {"category_id": 15, "poly": [296.0, 953.0, 1402.0, 953.0, 1402.0, 985.0, 296.0, 985.0], "score": 0.99, "text": "in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes"}, {"category_id": 15, "poly": [296.0, 985.0, 1404.0, 985.0, 1404.0, 1018.0, 296.0, 1018.0], "score": 0.98, "text": "it more difficult to learn dependencies between distant positions [11]. In the Transformer this is"}, {"category_id": 15, "poly": [296.0, 1015.0, 1404.0, 1015.0, 1404.0, 1047.0, 296.0, 1047.0], "score": 0.99, "text": "reduced to a constant number of operations, albeit at the cost of reduced effective resolution due"}, {"category_id": 15, "poly": [296.0, 1045.0, 1407.0, 1045.0, 1407.0, 1077.0, 296.0, 1077.0], "score": 0.99, "text": "to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as"}, {"category_id": 15, "poly": [293.0, 1075.0, 570.0, 1072.0, 571.0, 1104.0, 294.0, 1107.0], "score": 0.98, "text": "described in section 3.2."}, {"category_id": 15, "poly": [291.0, 761.0, 529.0, 761.0, 529.0, 807.0, 291.0, 807.0], "score": 0.96, "text": "2 Background"}, {"category_id": 15, "poly": [293.0, 1526.0, 631.0, 1526.0, 631.0, 1565.0, 293.0, 1565.0], "score": 0.99, "text": "3 Model Architecture"}, {"category_id": 15, "poly": [291.0, 1886.0, 709.0, 1888.0, 709.0, 1927.0, 291.0, 1925.0], "score": 0.97, "text": " 3.1  Encoder and Decoder Stacks"}, {"category_id": 15, "poly": [296.0, 463.0, 1402.0, 463.0, 1402.0, 495.0, 296.0, 495.0], "score": 0.99, "text": "Attention mechanisms have become an integral part of compelling sequence modeling and transduc-"}, {"category_id": 15, "poly": [296.0, 495.0, 1402.0, 495.0, 1402.0, 527.0, 296.0, 527.0], "score": 0.99, "text": "tion models in various tasks, allowing modeling of dependencies without regard to their distance in"}, {"category_id": 15, "poly": [296.0, 525.0, 1407.0, 525.0, 1407.0, 557.0, 296.0, 557.0], "score": 1.0, "text": "the input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms "}, {"category_id": 15, "poly": [296.0, 555.0, 845.0, 555.0, 845.0, 587.0, 296.0, 587.0], "score": 0.99, "text": "are used in conjunction with a recurrent network."}, {"category_id": 15, "poly": [293.0, 596.0, 1404.0, 596.0, 1404.0, 635.0, 293.0, 635.0], "score": 0.99, "text": " In this work we propose the Transformer, a model architecture eschewing recurrence and instead"}, {"category_id": 15, "poly": [296.0, 630.0, 1400.0, 630.0, 1400.0, 660.0, 296.0, 660.0], "score": 1.0, "text": "relying entirely on an attention mechanism to draw global dependencies between input and output"}, {"category_id": 15, "poly": [296.0, 658.0, 1407.0, 660.0, 1407.0, 692.0, 296.0, 690.0], "score": 0.99, "text": "The Transformer allows for significantly more parallelization and can reach a new state of the art in"}, {"category_id": 15, "poly": [293.0, 690.0, 1252.0, 690.0, 1252.0, 722.0, 293.0, 722.0], "score": 0.98, "text": " translation quality after being trained for as little as twelve hours on eight P100 GPUs"}, {"category_id": 15, "poly": [296.0, 1118.0, 1404.0, 1118.0, 1404.0, 1157.0, 296.0, 1157.0], "score": 0.99, "text": "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions"}, {"category_id": 15, "poly": [298.0, 1153.0, 1402.0, 1153.0, 1402.0, 1182.0, 298.0, 1182.0], "score": 0.98, "text": "of a single sequence in order to compute a representation of the sequence. Self-attention has been"}, {"category_id": 15, "poly": [296.0, 1180.0, 1407.0, 1183.0, 1407.0, 1215.0, 296.0, 1212.0], "score": 0.99, "text": "used successfully in a variety of tasks including reading comprehension, abstractive summarization,"}, {"category_id": 15, "poly": [296.0, 1215.0, 1289.0, 1215.0, 1289.0, 1244.0, 296.0, 1244.0], "score": 0.98, "text": "textual entailment and learning task-independent sentence representations [4, 22, 23, 19]."}, {"category_id": 15, "poly": [298.0, 1597.0, 1404.0, 1597.0, 1404.0, 1629.0, 298.0, 1629.0], "score": 0.99, "text": "Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29]."}, {"category_id": 15, "poly": [296.0, 1716.0, 1347.0, 1716.0, 1347.0, 1749.0, 296.0, 1749.0], "score": 0.99, "text": "[9], consuming the previously generated symbols as additional input when generating the next."}, {"category_id": 15, "poly": [293.0, 1657.0, 646.0, 1657.0, 646.0, 1689.0, 293.0, 1689.0], "score": 0.99, "text": "of continuous representations"}, {"category_id": 15, "poly": [293.0, 1625.0, 1104.0, 1627.0, 1104.0, 1659.0, 293.0, 1657.0], "score": 0.99, "text": " Here, the encoder maps an input sequence of symbol representations "}, {"category_id": 15, "poly": [1235.0, 1625.0, 1404.0, 1627.0, 1404.0, 1659.0, 1235.0, 1657.0], "score": 1.0, "text": "toasequence"}, {"category_id": 15, "poly": [296.0, 1684.0, 409.0, 1684.0, 409.0, 1723.0, 296.0, 1723.0], "score": 1.0, "text": "sequence"}, {"category_id": 15, "poly": [541.0, 1684.0, 1409.0, 1684.0, 1409.0, 1723.0, 541.0, 1723.0], "score": 0.98, "text": " of symbols one element at a time. At each step the model is auto-regressive"}, {"category_id": 15, "poly": [835.0, 1657.0, 934.0, 1657.0, 934.0, 1689.0, 835.0, 1689.0], "score": 0.98, "text": ".Given"}, {"category_id": 15, "poly": [955.0, 1657.0, 1400.0, 1657.0, 1400.0, 1689.0, 955.0, 1689.0], "score": 0.98, "text": ", the decoder then generates an output"}, {"category_id": 15, "poly": [298.0, 1765.0, 1400.0, 1765.0, 1400.0, 1797.0, 298.0, 1797.0], "score": 0.99, "text": "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully"}, {"category_id": 15, "poly": [298.0, 1794.0, 1402.0, 1794.0, 1402.0, 1826.0, 298.0, 1826.0], "score": 0.99, "text": "connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,"}, {"category_id": 15, "poly": [296.0, 1826.0, 439.0, 1826.0, 439.0, 1859.0, 296.0, 1859.0], "score": 0.99, "text": "respectively."}, {"category_id": 15, "poly": [293.0, 1361.0, 1402.0, 1366.0, 1402.0, 1398.0, 293.0, 1393.0], "score": 0.99, "text": " To the best of our knowledge, however, the Transformer is the first transduction model relying"}, {"category_id": 15, "poly": [298.0, 1396.0, 1402.0, 1396.0, 1402.0, 1425.0, 298.0, 1425.0], "score": 1.0, "text": "entirely on self-attention to compute representations of its input and output without using sequence-"}, {"category_id": 15, "poly": [300.0, 1425.0, 1404.0, 1425.0, 1404.0, 1455.0, 300.0, 1455.0], "score": 0.99, "text": "aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate"}, {"category_id": 15, "poly": [300.0, 1455.0, 1157.0, 1455.0, 1157.0, 1487.0, 300.0, 1487.0], "score": 0.99, "text": "self-attention and discuss its advantages over models such as [14, 15] and [8]."}, {"category_id": 15, "poly": [293.0, 1251.0, 1404.0, 1254.0, 1404.0, 1293.0, 293.0, 1290.0], "score": 0.98, "text": " End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-"}, {"category_id": 15, "poly": [298.0, 1290.0, 1402.0, 1290.0, 1402.0, 1320.0, 298.0, 1320.0], "score": 0.99, "text": "aligned recurrence and have been shown to perform well on simple-language question answering and"}, {"category_id": 15, "poly": [298.0, 1320.0, 633.0, 1320.0, 633.0, 1352.0, 298.0, 1352.0], "score": 1.0, "text": "language modeling tasks [28]."}, {"category_id": 15, "poly": [298.0, 1980.0, 1402.0, 1980.0, 1402.0, 2012.0, 298.0, 2012.0], "score": 0.99, "text": "sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-"}, {"category_id": 15, "poly": [298.0, 1948.0, 890.0, 1948.0, 890.0, 1980.0, 298.0, 1980.0], "score": 0.99, "text": "Encoder: The encoder is composed of a stack of"}, {"category_id": 15, "poly": [979.0, 1948.0, 1402.0, 1948.0, 1402.0, 1980.0, 979.0, 1980.0], "score": 0.99, "text": "identical layers. Each layer has two"}, {"category_id": 15, "poly": [843.0, 2069.0, 859.0, 2069.0, 859.0, 2081.0, 843.0, 2081.0], "score": 0.68, "text": "2"}], "page_info": {"page_no": 1, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 1, "poly": [298.0679626464844, 1409.51611328125, 1403.9415283203125, 1409.51611328125, 1403.9415283203125, 1626.1558837890625, 298.0679626464844, 1626.1558837890625], "score": 0.9999946355819702}, {"category_id": 3, "poly": [545.877685546875, 200.9801025390625, 1159.8466796875, 200.9801025390625, 1159.8466796875, 1099.8997802734375, 545.877685546875, 1099.8997802734375], "score": 0.9999921321868896}, {"category_id": 1, "poly": [299.3133239746094, 1215.5908203125, 1402.8709716796875, 1215.5908203125, 1402.8709716796875, 1372.1065673828125, 299.3133239746094, 1372.1065673828125], "score": 0.9999898672103882}, {"category_id": 0, "poly": [299.068603515625, 1889.838134765625, 734.2828369140625, 1889.838134765625, 734.2828369140625, 1922.662841796875, 299.068603515625, 1922.662841796875], "score": 0.9999720454216003}, {"category_id": 1, "poly": [299.9129943847656, 1728.057861328125, 1403.17333984375, 1728.057861328125, 1403.17333984375, 1849.7774658203125, 299.9129943847656, 1849.7774658203125], "score": 0.9999677538871765}, {"category_id": 0, "poly": [299.31207275390625, 1668.395751953125, 477.6578674316406, 1668.395751953125, 477.6578674316406, 1700.180419921875, 299.31207275390625, 1700.180419921875], "score": 0.9999634623527527}, {"category_id": 4, "poly": [579.8930053710938, 1121.896728515625, 1117.9776611328125, 1121.896728515625, 1117.9776611328125, 1154.082275390625, 579.8930053710938, 1154.082275390625], "score": 0.9999573230743408}, {"category_id": 1, "poly": [299.2904357910156, 1944.7073974609375, 1404.1832275390625, 1944.7073974609375, 1404.1832275390625, 2009.07470703125, 299.2904357910156, 2009.07470703125], "score": 0.9999501705169678}, {"category_id": 2, "poly": [842.4070434570312, 2062.14697265625, 858.1945190429688, 2062.14697265625, 858.1945190429688, 2083.486572265625, 842.4070434570312, 2083.486572265625], "score": 0.9998016357421875}, {"category_id": 13, "poly": [714, 1339, 860, 1339, 860, 1369, 714, 1369], "score": 0.9, "latex": "d_{\\mathrm{model}}=512"}, {"category_id": 13, "poly": [904, 1412, 983, 1412, 983, 1438, 904, 1438], "score": 0.9, "latex": "N=6"}, {"category_id": 13, "poly": [960, 1977, 989, 1977, 989, 2006, 960, 2006], "score": 0.88, "latex": "d_{v}"}, {"category_id": 13, "poly": [642, 1977, 672, 1977, 672, 2006, 642, 2006], "score": 0.88, "latex": "d_{k}"}, {"category_id": 13, "poly": [846, 1278, 883, 1278, 883, 1310, 846, 1310], "score": 0.72, "latex": "(x)"}, {"category_id": 13, "poly": [563, 1595, 576, 1595, 576, 1620, 563, 1620], "score": 0.72, "latex": "i"}, {"category_id": 13, "poly": [353, 1277, 648, 1277, 648, 1311, 353, 1311], "score": 0.65, "latex": "\\therefore{\\mathrm{Norm}}({\\bar{x}}+{\\mathrm{Sublayer}}(x))"}, {"category_id": 13, "poly": [1249, 1595, 1262, 1595, 1262, 1620, 1249, 1620], "score": 0.56, "latex": "i"}, {"category_id": 13, "poly": [740, 1277, 883, 1277, 883, 1310, 740, 1310], "score": 0.26, "latex": "\\operatorname{Subset}(x)"}, {"category_id": 15, "poly": [296.0, 1441.0, 1404.0, 1441.0, 1404.0, 1474.0, 296.0, 1474.0], "score": 0.99, "text": "sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head"}, {"category_id": 15, "poly": [296.0, 1474.0, 1402.0, 1474.0, 1402.0, 1506.0, 296.0, 1506.0], "score": 0.99, "text": "attention over the output of the encoder stack. Similar to the encoder, we employ residual connections"}, {"category_id": 15, "poly": [298.0, 1503.0, 1402.0, 1503.0, 1402.0, 1535.0, 298.0, 1535.0], "score": 0.99, "text": "around each of the sub-layers, followed by layer normalization. We also modify the self-attention"}, {"category_id": 15, "poly": [296.0, 1535.0, 1404.0, 1535.0, 1404.0, 1565.0, 296.0, 1565.0], "score": 0.98, "text": "sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This"}, {"category_id": 15, "poly": [293.0, 1563.0, 1402.0, 1563.0, 1402.0, 1593.0, 293.0, 1593.0], "score": 0.99, "text": " masking, combined with fact that the output embeddings are offset by one position, ensures that the"}, {"category_id": 15, "poly": [298.0, 1412.0, 903.0, 1412.0, 903.0, 1444.0, 298.0, 1444.0], "score": 0.98, "text": "Decoder: The decoder is also composed of a stack of"}, {"category_id": 15, "poly": [984.0, 1412.0, 1404.0, 1412.0, 1404.0, 1444.0, 984.0, 1444.0], "score": 0.99, "text": "identical layers. In addition to the two"}, {"category_id": 15, "poly": [296.0, 1597.0, 562.0, 1597.0, 562.0, 1627.0, 296.0, 1627.0], "score": 0.97, "text": "predictions for position"}, {"category_id": 15, "poly": [577.0, 1597.0, 1248.0, 1597.0, 1248.0, 1627.0, 577.0, 1627.0], "score": 1.0, "text": "can depend only on the known outputs at positions less than"}, {"category_id": 15, "poly": [298.0, 1219.0, 1404.0, 1219.0, 1404.0, 1249.0, 298.0, 1249.0], "score": 0.99, "text": "wise fully connected feed-forward network. We employ a residual connection [10] around each of"}, {"category_id": 15, "poly": [296.0, 1251.0, 1404.0, 1251.0, 1404.0, 1281.0, 296.0, 1281.0], "score": 0.98, "text": "the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is"}, {"category_id": 15, "poly": [293.0, 1306.0, 1404.0, 1311.0, 1404.0, 1343.0, 293.0, 1338.0], "score": 0.98, "text": " itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding"}, {"category_id": 15, "poly": [291.0, 1338.0, 713.0, 1336.0, 713.0, 1375.0, 291.0, 1377.0], "score": 1.0, "text": "layers, produce outputs of dimension"}, {"category_id": 15, "poly": [884.0, 1279.0, 1404.0, 1279.0, 1404.0, 1311.0, 884.0, 1311.0], "score": 1.0, "text": "is the function implemented by the sub-layer"}, {"category_id": 15, "poly": [296.0, 1279.0, 352.0, 1279.0, 352.0, 1311.0, 296.0, 1311.0], "score": 0.99, "text": "Laye"}, {"category_id": 15, "poly": [649.0, 1279.0, 739.0, 1279.0, 739.0, 1311.0, 649.0, 1311.0], "score": 0.94, "text": "\uff0cwhere"}, {"category_id": 15, "poly": [298.0, 1893.0, 737.0, 1893.0, 737.0, 1925.0, 298.0, 1925.0], "score": 0.99, "text": "3.2.1 Scaled Dot-Product Attention"}, {"category_id": 15, "poly": [296.0, 1728.0, 1402.0, 1730.0, 1402.0, 1762.0, 296.0, 1760.0], "score": 0.99, "text": "An attention function can be described as mapping a query and a set of key-value pairs to an output,"}, {"category_id": 15, "poly": [298.0, 1762.0, 1402.0, 1762.0, 1402.0, 1792.0, 298.0, 1792.0], "score": 1.0, "text": "where the query, keys, values, and output are all vectors. The output is computed as a weighted sum"}, {"category_id": 15, "poly": [298.0, 1790.0, 1402.0, 1790.0, 1402.0, 1822.0, 298.0, 1822.0], "score": 0.99, "text": "of the values, where the weight assigned to each value is computed by a compatibility function of the"}, {"category_id": 15, "poly": [296.0, 1822.0, 679.0, 1822.0, 679.0, 1854.0, 296.0, 1854.0], "score": 0.99, "text": "query with the corresponding key."}, {"category_id": 15, "poly": [291.0, 1666.0, 478.0, 1666.0, 478.0, 1705.0, 291.0, 1705.0], "score": 0.96, "text": "3.2 Attention"}, {"category_id": 15, "poly": [582.0, 1123.0, 1116.0, 1123.0, 1116.0, 1155.0, 582.0, 1155.0], "score": 1.0, "text": "Figure 1: The Transformer - model architecture."}, {"category_id": 15, "poly": [296.0, 1948.0, 1400.0, 1948.0, 1400.0, 1980.0, 296.0, 1980.0], "score": 0.99, "text": "We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of"}, {"category_id": 15, "poly": [990.0, 1980.0, 1400.0, 1978.0, 1400.0, 2010.0, 990.0, 2012.0], "score": 0.99, "text": ".We compute the dot products of the"}, {"category_id": 15, "poly": [296.0, 1980.0, 641.0, 1978.0, 641.0, 2010.0, 296.0, 2012.0], "score": 0.97, "text": "queries and keys of dimension"}, {"category_id": 15, "poly": [673.0, 1980.0, 959.0, 1978.0, 959.0, 2010.0, 673.0, 2012.0], "score": 0.95, "text": ",and values of dimension"}], "page_info": {"page_no": 2, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 8, "poly": [611.02978515625, 1105.1015625, 1091.8082275390625, 1105.1015625, 1091.8082275390625, 1178.942626953125, 611.02978515625, 1178.942626953125], "score": 0.9999861717224121}, {"category_id": 1, "poly": [299.87908935546875, 1213.585205078125, 1402.8197021484375, 1213.585205078125, 1402.8197021484375, 1405.4051513671875, 299.87908935546875, 1405.4051513671875], "score": 0.9999858736991882}, {"category_id": 1, "poly": [300.20501708984375, 1646.3416748046875, 1402.9556884765625, 1646.3416748046875, 1402.9556884765625, 1830.1932373046875, 300.20501708984375, 1830.1932373046875], "score": 0.9999849200248718}, {"category_id": 2, "poly": [296.4795837402344, 1946.477783203125, 1403.6099853515625, 1946.477783203125, 1403.6099853515625, 2009.856689453125, 296.4795837402344, 2009.856689453125], "score": 0.9999666213989258}, {"category_id": 1, "poly": [299.9501647949219, 963.7103271484375, 1401.3802490234375, 963.7103271484375, 1401.3802490234375, 1054.152587890625, 299.9501647949219, 1054.152587890625], "score": 0.9999663233757019}, {"category_id": 1, "poly": [299.7311706542969, 1419.0255126953125, 1401.95703125, 1419.0255126953125, 1401.95703125, 1544.958740234375, 299.7311706542969, 1544.958740234375], "score": 0.9999639987945557}, {"category_id": 0, "poly": [300.5436706542969, 1591.647705078125, 643.0038452148438, 1591.647705078125, 643.0038452148438, 1620.498779296875, 300.5436706542969, 1620.498779296875], "score": 0.9999274611473083}, {"category_id": 9, "poly": [1367.3489990234375, 1130.5107421875, 1400.0888671875, 1130.5107421875, 1400.0888671875, 1157.16845703125, 1367.3489990234375, 1157.16845703125], "score": 0.99986732006073}, {"category_id": 1, "poly": [299.42913818359375, 1843.73974609375, 1402.6688232421875, 1843.73974609375, 1402.6688232421875, 1905.72998046875, 299.42913818359375, 1905.72998046875], "score": 0.999819278717041}, {"category_id": 1, "poly": [299.39178466796875, 760.7088623046875, 1402.5919189453125, 760.7088623046875, 1402.5919189453125, 819.5951538085938, 299.39178466796875, 819.5951538085938], "score": 0.999818742275238}, {"category_id": 1, "poly": [300.2699279785156, 889.2484130859375, 1402.14794921875, 889.2484130859375, 1402.14794921875, 947.1972045898438, 300.2699279785156, 947.1972045898438], "score": 0.9997832179069519}, {"category_id": 3, "poly": [466.83306884765625, 243.791015625, 1295.280517578125, 243.791015625, 1295.280517578125, 670.77490234375, 466.83306884765625, 670.77490234375], "score": 0.999677836894989}, {"category_id": 2, "poly": [841.5859985351562, 2062.7763671875, 857.977294921875, 2062.7763671875, 857.977294921875, 2083.67041015625, 841.5859985351562, 2083.67041015625], "score": 0.9969159960746765}, {"category_id": 2, "poly": [1007.3978271484375, 196.65109252929688, 1254.2314453125, 196.65109252929688, 1254.2314453125, 223.39102172851562, 1007.3978271484375, 223.39102172851562], "score": 0.9845589399337769}, {"category_id": 2, "poly": [409.2307434082031, 197.777099609375, 743.06982421875, 197.777099609375, 743.06982421875, 224.41062927246094, 409.2307434082031, 224.41062927246094], "score": 0.8755929470062256}, {"category_id": 0, "poly": [409.6474609375, 197.9058380126953, 742.895263671875, 197.9058380126953, 742.895263671875, 224.57318115234375, 409.6474609375, 224.57318115234375], "score": 0.37341558933258057}, {"category_id": 3, "poly": [950.4749145507812, 230.3736114501953, 1293.6224365234375, 230.3736114501953, 1293.6224365234375, 668.5567016601562, 950.4749145507812, 668.5567016601562], "score": 0.3237869143486023}, {"category_id": 13, "poly": [917, 1974, 1105, 1974, 1105, 2010, 917, 2010], "score": 0.93, "latex": "\\begin{array}{r}{q\\cdot k=\\sum_{i=1}^{d_{k}}q_{i}k_{i}}\\end{array}"}, {"category_id": 14, "poly": [606, 1103, 1094, 1103, 1094, 1181, 606, 1181], "score": 0.93, "latex": "\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}(\\frac{Q K^{T}}{\\sqrt{d_{k}}})V"}, {"category_id": 13, "poly": [693, 885, 746, 885, 746, 919, 693, 919], "score": 0.92, "latex": "\\sqrt{d_{k}}"}, {"category_id": 13, "poly": [1220, 1507, 1269, 1507, 1269, 1551, 1220, 1551], "score": 0.92, "latex": "\\textstyle{\\frac{1}{\\sqrt{d_{k}}}}"}, {"category_id": 13, "poly": [329, 1273, 379, 1273, 379, 1317, 329, 1317], "score": 0.92, "latex": "\\frac{1}{\\sqrt{d_{k}}}"}, {"category_id": 13, "poly": [905, 1647, 971, 1647, 971, 1676, 905, 1676], "score": 0.91, "latex": "d_{\\mathrm{model}}"}, {"category_id": 13, "poly": [585, 1420, 616, 1420, 616, 1449, 585, 1449], "score": 0.89, "latex": "d_{k}"}, {"category_id": 13, "poly": [651, 1708, 681, 1708, 681, 1737, 651, 1737], "score": 0.88, "latex": "d_{v}"}, {"category_id": 13, "poly": [1226, 1737, 1256, 1737, 1256, 1767, 1226, 1767], "score": 0.88, "latex": "d_{v}"}, {"category_id": 13, "poly": [932, 1450, 963, 1450, 963, 1479, 932, 1479], "score": 0.88, "latex": "d_{k}"}, {"category_id": 13, "poly": [1365, 1978, 1393, 1978, 1393, 2005, 1365, 2005], "score": 0.87, "latex": "d_{k}"}, {"category_id": 13, "poly": [298, 1480, 328, 1480, 328, 1509, 298, 1509], "score": 0.87, "latex": "d_{k}"}, {"category_id": 13, "poly": [1142, 995, 1170, 995, 1170, 1021, 1142, 1021], "score": 0.84, "latex": "K"}, {"category_id": 13, "poly": [446, 995, 471, 995, 471, 1025, 446, 1025], "score": 0.82, "latex": "Q"}, {"category_id": 13, "poly": [1221, 995, 1246, 995, 1246, 1021, 1221, 1021], "score": 0.81, "latex": "V"}, {"category_id": 13, "poly": [1141, 1947, 1159, 1947, 1159, 1971, 1141, 1971], "score": 0.81, "latex": "k"}, {"category_id": 13, "poly": [527, 1707, 557, 1707, 557, 1737, 527, 1737], "score": 0.81, "latex": "d_{k}"}, {"category_id": 13, "poly": [1082, 1951, 1097, 1951, 1097, 1974, 1082, 1974], "score": 0.79, "latex": "q"}, {"category_id": 13, "poly": [568, 1707, 599, 1707, 599, 1737, 568, 1737], "score": 0.78, "latex": "d_{k}"}, {"category_id": 13, "poly": [1062, 1678, 1082, 1678, 1082, 1704, 1062, 1704], "score": 0.77, "latex": "h"}, {"category_id": 15, "poly": [298.0, 1217.0, 1402.0, 1217.0, 1402.0, 1249.0, 298.0, 1249.0], "score": 0.99, "text": "The two most commonly used attention functions are additive attention [2], and dot-product (multi-"}, {"category_id": 15, "poly": [293.0, 1247.0, 1400.0, 1247.0, 1400.0, 1279.0, 293.0, 1279.0], "score": 0.99, "text": " plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor"}, {"category_id": 15, "poly": [296.0, 1313.0, 1402.0, 1313.0, 1402.0, 1345.0, 296.0, 1345.0], "score": 0.99, "text": "a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is"}, {"category_id": 15, "poly": [296.0, 1345.0, 1402.0, 1345.0, 1402.0, 1377.0, 296.0, 1377.0], "score": 1.0, "text": "much faster and more space-efficient in practice, since it can be implemented using highly optimized"}, {"category_id": 15, "poly": [296.0, 1377.0, 605.0, 1377.0, 605.0, 1407.0, 296.0, 1407.0], "score": 0.99, "text": "matrix multiplication code."}, {"category_id": 15, "poly": [298.0, 1769.0, 1404.0, 1769.0, 1404.0, 1801.0, 298.0, 1801.0], "score": 0.99, "text": "output values. These are concatenated and once again projected, resulting in the final values, as"}, {"category_id": 15, "poly": [296.0, 1801.0, 531.0, 1801.0, 531.0, 1833.0, 296.0, 1833.0], "score": 1.0, "text": "depicted in Figure 2."}, {"category_id": 15, "poly": [293.0, 1643.0, 904.0, 1645.0, 904.0, 1684.0, 293.0, 1682.0], "score": 0.96, "text": " Instead of performing a single attention function with"}, {"category_id": 15, "poly": [972.0, 1643.0, 1404.0, 1645.0, 1404.0, 1684.0, 972.0, 1682.0], "score": 0.98, "text": "-dimensional keys, values and queries,"}, {"category_id": 15, "poly": [682.0, 1710.0, 1404.0, 1710.0, 1404.0, 1742.0, 682.0, 1742.0], "score": 0.98, "text": " dimensions, respectively. On each of these projected versions of"}, {"category_id": 15, "poly": [298.0, 1739.0, 1225.0, 1739.0, 1225.0, 1771.0, 298.0, 1771.0], "score": 1.0, "text": "queries, keys and values we then perform the attention function in parallel, yielding"}, {"category_id": 15, "poly": [1257.0, 1739.0, 1402.0, 1739.0, 1402.0, 1771.0, 1257.0, 1771.0], "score": 1.0, "text": "-dimensional"}, {"category_id": 15, "poly": [296.0, 1710.0, 526.0, 1710.0, 526.0, 1742.0, 296.0, 1742.0], "score": 0.98, "text": "linear projections to"}, {"category_id": 15, "poly": [558.0, 1710.0, 567.0, 1710.0, 567.0, 1742.0, 558.0, 1742.0], "score": 0.91, "text": "\uff0c"}, {"category_id": 15, "poly": [600.0, 1710.0, 650.0, 1710.0, 650.0, 1742.0, 600.0, 1742.0], "score": 1.0, "text": "and"}, {"category_id": 15, "poly": [293.0, 1673.0, 1061.0, 1675.0, 1061.0, 1714.0, 293.0, 1712.0], "score": 0.99, "text": "we found it beneficial to linearly project the queries, keys and values"}, {"category_id": 15, "poly": [1083.0, 1673.0, 1404.0, 1675.0, 1404.0, 1714.0, 1083.0, 1712.0], "score": 0.99, "text": "times with different, learned"}, {"category_id": 15, "poly": [298.0, 1978.0, 916.0, 1978.0, 916.0, 2017.0, 298.0, 2017.0], "score": 0.99, "text": "variables with mean O and variance 1. Then their dot product,"}, {"category_id": 15, "poly": [1106.0, 1978.0, 1364.0, 1978.0, 1364.0, 2017.0, 1106.0, 2017.0], "score": 0.95, "text": ", has mean O and variance"}, {"category_id": 15, "poly": [1160.0, 1941.0, 1404.0, 1943.0, 1404.0, 1982.0, 1160.0, 1980.0], "score": 0.99, "text": "are independent random"}, {"category_id": 15, "poly": [330.0, 1941.0, 1081.0, 1943.0, 1081.0, 1982.0, 330.0, 1980.0], "score": 0.99, "text": "4To illustrate why the dot products get large, assume that the components of"}, {"category_id": 15, "poly": [1098.0, 1941.0, 1140.0, 1943.0, 1140.0, 1982.0, 1098.0, 1980.0], "score": 1.0, "text": "and"}, {"category_id": 15, "poly": [293.0, 962.0, 1404.0, 967.0, 1404.0, 999.0, 293.0, 995.0], "score": 1.0, "text": " In practice, we compute the attention function on a set of queries simultaneously, packed together"}, {"category_id": 15, "poly": [296.0, 1024.0, 573.0, 1027.0, 573.0, 1059.0, 296.0, 1056.0], "score": 0.99, "text": "the matrix of outputs as:"}, {"category_id": 15, "poly": [293.0, 997.0, 445.0, 997.0, 445.0, 1029.0, 293.0, 1029.0], "score": 0.98, "text": "into a matrix"}, {"category_id": 15, "poly": [472.0, 997.0, 1141.0, 997.0, 1141.0, 1029.0, 472.0, 1029.0], "score": 0.99, "text": ". The keys and values are also packed together into matrices"}, {"category_id": 15, "poly": [1171.0, 997.0, 1220.0, 997.0, 1220.0, 1029.0, 1171.0, 1029.0], "score": 1.0, "text": "and"}, {"category_id": 15, "poly": [1247.0, 997.0, 1400.0, 997.0, 1400.0, 1029.0, 1247.0, 1029.0], "score": 0.94, "text": ".Wecompute"}, {"category_id": 15, "poly": [298.0, 1421.0, 584.0, 1421.0, 584.0, 1453.0, 298.0, 1453.0], "score": 0.97, "text": "While for small values of"}, {"category_id": 15, "poly": [617.0, 1421.0, 1402.0, 1421.0, 1402.0, 1453.0, 617.0, 1453.0], "score": 0.98, "text": "the two mechanisms perform similarly, additive attention outperforms"}, {"category_id": 15, "poly": [298.0, 1453.0, 931.0, 1453.0, 931.0, 1483.0, 298.0, 1483.0], "score": 0.99, "text": "dot product attention without scaling for larger values of"}, {"category_id": 15, "poly": [964.0, 1453.0, 1402.0, 1453.0, 1402.0, 1483.0, 964.0, 1483.0], "score": 0.97, "text": "[3]. We suspect that for large values of"}, {"category_id": 15, "poly": [329.0, 1483.0, 1402.0, 1483.0, 1402.0, 1512.0, 329.0, 1512.0], "score": 0.99, "text": ", the dot products grow large in magnitude, pushing the softmax function into regions where it has"}, {"category_id": 15, "poly": [294.0, 1586.0, 647.0, 1588.0, 647.0, 1627.0, 293.0, 1625.0], "score": 0.97, "text": "3.2.2  Multi-Head Attention"}, {"category_id": 15, "poly": [298.0, 1845.0, 1402.0, 1845.0, 1402.0, 1877.0, 298.0, 1877.0], "score": 0.99, "text": "Multi-head attention allows the model to jointly attend to information from different representation"}, {"category_id": 15, "poly": [298.0, 1877.0, 1236.0, 1877.0, 1236.0, 1909.0, 298.0, 1909.0], "score": 1.0, "text": "subspaces at different positions. With a single attention head, averaging inhibits this."}, {"category_id": 15, "poly": [296.0, 759.0, 1397.0, 759.0, 1397.0, 791.0, 296.0, 791.0], "score": 0.99, "text": "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several"}, {"category_id": 15, "poly": [296.0, 795.0, 691.0, 795.0, 691.0, 825.0, 296.0, 825.0], "score": 0.99, "text": "attention layers running in parallel."}, {"category_id": 15, "poly": [298.0, 924.0, 374.0, 924.0, 374.0, 949.0, 298.0, 949.0], "score": 0.99, "text": "values."}, {"category_id": 15, "poly": [296.0, 891.0, 692.0, 891.0, 692.0, 924.0, 296.0, 924.0], "score": 0.99, "text": "query with all keys, divide each by"}, {"category_id": 15, "poly": [747.0, 891.0, 1402.0, 891.0, 1402.0, 924.0, 747.0, 924.0], "score": 0.99, "text": ", and apply a softmax function to obtain the weights on the"}, {"category_id": 15, "poly": [1005.0, 190.0, 1254.0, 195.0, 1254.0, 236.0, 1004.0, 231.0], "score": 0.95, "text": "Multi-Head Attention"}, {"category_id": 15, "poly": [409.0, 197.0, 741.0, 197.0, 741.0, 229.0, 409.0, 229.0], "score": 0.98, "text": "Scaled Dot-Product Attention"}, {"category_id": 15, "poly": [409.0, 197.0, 741.0, 197.0, 741.0, 229.0, 409.0, 229.0], "score": 0.98, "text": "Scaled Dot-Product Attention"}], "page_info": {"page_no": 3, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 0, "poly": [297.8894958496094, 614.6411743164062, 846.7774658203125, 614.6411743164062, 846.7774658203125, 648.5343627929688, 297.8894958496094, 648.5343627929688], "score": 0.9999958872795105}, {"category_id": 1, "poly": [294.4927062988281, 489.8787841796875, 1404.7398681640625, 489.8787841796875, 1404.7398681640625, 586.0765991210938, 294.4927062988281, 586.0765991210938], "score": 0.9999850988388062}, {"category_id": 1, "poly": [362.0790710449219, 720.2603149414062, 1407.287841796875, 720.2603149414062, 1407.287841796875, 1175.4931640625, 362.0790710449219, 1175.4931640625], "score": 0.9999839067459106}, {"category_id": 1, "poly": [294.9356994628906, 1260.9674072265625, 1405.0113525390625, 1260.9674072265625, 1405.0113525390625, 1356.471435546875, 294.9356994628906, 1356.471435546875], "score": 0.9999815821647644}, {"category_id": 0, "poly": [296.4703063964844, 1614.338623046875, 670.900390625, 1614.338623046875, 670.900390625, 1647.9310302734375, 296.4703063964844, 1647.9310302734375], "score": 0.9999780654907227}, {"category_id": 8, "poly": [625.826171875, 1395.227783203125, 1075.5452880859375, 1395.227783203125, 1075.5452880859375, 1435.3724365234375, 625.826171875, 1435.3724365234375], "score": 0.9999734163284302}, {"category_id": 0, "poly": [297.922119140625, 1857.7362060546875, 601.9944458007812, 1857.7362060546875, 601.9944458007812, 1892.796142578125, 297.922119140625, 1892.796142578125], "score": 0.9999725222587585}, {"category_id": 1, "poly": [294.75567626953125, 1912.5596923828125, 1406.25, 1912.5596923828125, 1406.25, 2010.6641845703125, 294.75567626953125, 2010.6641845703125], "score": 0.9999601244926453}, {"category_id": 1, "poly": [294.8651428222656, 1453.6380615234375, 1405.3310546875, 1453.6380615234375, 1405.3310546875, 1580.9327392578125, 294.8651428222656, 1580.9327392578125], "score": 0.9999573230743408}, {"category_id": 0, "poly": [297.1821594238281, 1206.177734375, 818.6870727539062, 1206.177734375, 818.6870727539062, 1239.2161865234375, 297.1821594238281, 1239.2161865234375], "score": 0.9999512434005737}, {"category_id": 1, "poly": [295.0391540527344, 1668.333984375, 1407.4581298828125, 1668.333984375, 1407.4581298828125, 1827.3287353515625, 295.0391540527344, 1827.3287353515625], "score": 0.9999508857727051}, {"category_id": 1, "poly": [295.0030212402344, 406.4055480957031, 1402.31298828125, 406.4055480957031, 1402.31298828125, 476.4635925292969, 295.0030212402344, 476.4635925292969], "score": 0.9999387264251709}, {"category_id": 9, "poly": [1366.4088134765625, 1400.2781982421875, 1400.26416015625, 1400.2781982421875, 1400.26416015625, 1430.1253662109375, 1366.4088134765625, 1430.1253662109375], "score": 0.9999281764030457}, {"category_id": 1, "poly": [297.8404846191406, 665.5425415039062, 1038.666259765625, 665.5425415039062, 1038.666259765625, 699.0245971679688, 297.8404846191406, 699.0245971679688], "score": 0.9995660781860352}, {"category_id": 2, "poly": [841.3950805664062, 2060.49462890625, 859.3304443359375, 2060.49462890625, 859.3304443359375, 2084.57958984375, 841.3950805664062, 2084.57958984375], "score": 0.9985367059707642}, {"category_id": 8, "poly": [514.3692626953125, 248.40939331054688, 1188.60205078125, 248.40939331054688, 1188.60205078125, 338.6997375488281, 514.3692626953125, 338.6997375488281], "score": 0.9490617513656616}, {"category_id": 13, "poly": [298, 1547, 434, 1547, 434, 1583, 298, 1583], "score": 0.95, "latex": "d_{f f}=2048"}, {"category_id": 13, "poly": [1309, 1790, 1399, 1790, 1399, 1824, 1309, 1824], "score": 0.92, "latex": "\\sqrt{d_{\\mathrm{model}}}"}, {"category_id": 13, "poly": [788, 1517, 943, 1517, 943, 1548, 788, 1548], "score": 0.91, "latex": "d_{\\mathrm{model}}\\,=\\,512"}, {"category_id": 13, "poly": [345, 442, 551, 442, 551, 473, 345, 473], "score": 0.91, "latex": "W^{O}\\in\\mathbb{R}^{h d_{v}\\times d_{\\mathrm{model}}}"}, {"category_id": 13, "poly": [297, 521, 590, 521, 590, 554, 297, 554], "score": 0.91, "latex": "d_{k}=d_{v}=d_{\\mathrm{model}}/h=64"}, {"category_id": 13, "poly": [836, 1702, 903, 1702, 903, 1732, 836, 1732], "score": 0.9, "latex": "d_{\\mathrm{model}}"}, {"category_id": 14, "poly": [626, 1396, 1074, 1396, 1074, 1434, 626, 1434], "score": 0.9, "latex": "\\mathrm{FFN}(x)=\\operatorname*{max}(0,x W_{1}+b_{1})W_{2}+b_{2}"}, {"category_id": 13, "poly": [591, 491, 677, 491, 677, 519, 591, 519], "score": 0.9, "latex": "h\\ =\\ 8"}, {"category_id": 13, "poly": [792, 405, 984, 405, 984, 445, 792, 445], "score": 0.88, "latex": "W_{i}^{Q}\\in\\mathbb{R}^{d_{\\mathrm{model}}\\times d_{k}}"}, {"category_id": 13, "poly": [1100, 1112, 1157, 1112, 1157, 1140, 1100, 1140], "score": 0.79, "latex": "-\\infty)"}, {"category_id": 14, "poly": [517, 251, 1150, 251, 1150, 287, 517, 287], "score": 0.51, "latex": "\\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head}_{1},...,\\mathrm{head}_{\\mathrm{h}})W^{O}"}, {"category_id": 14, "poly": [618, 295, 1176, 295, 1176, 335, 618, 335], "score": 0.46, "latex": "\\mathrm{where\\;head_{i}=A t e n t i o n(}Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{V})}"}, {"category_id": 13, "poly": [513, 251, 1159, 251, 1159, 287, 513, 287], "score": 0.37, "latex": "\\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head}_{1},...,\\mathrm{head}_{\\mathrm{h}})W^{O}"}, {"category_id": 13, "poly": [618, 295, 1176, 295, 1176, 334, 618, 334], "score": 0.37, "latex": "\\mathrm{where\\;head_{i}=A t e n t i o n(}Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{V})}"}, {"category_id": 13, "poly": [997, 407, 1193, 407, 1193, 445, 997, 445], "score": 0.27, "latex": "W_{i}^{K}\\in\\mathbb{R}^{d_{\\mathrm{model}}\\times d_{k}}"}, {"category_id": 14, "poly": [515, 252, 1173, 252, 1173, 339, 515, 339], "score": 0.25, "latex": "\\begin{array}{r}{\\mathrm{MultiHead}(Q,K,V)=\\mathrm{Concat}(\\mathrm{head}_{1},...,\\mathrm{head}_{\\mathrm{h}})W^{O}\\qquad\\qquad}\\\\ {\\mathrm{where~head}_{\\mathrm{i}}=\\mathrm{Attention}(Q W_{i}^{Q},K W_{i}^{K},V W_{i}^{V})}\\end{array}"}, {"category_id": 15, "poly": [296.0, 614.0, 845.0, 614.0, 845.0, 653.0, 296.0, 653.0], "score": 0.97, "text": "3.2.3  Applications of Attention in our Model"}, {"category_id": 15, "poly": [293.0, 552.0, 1026.0, 557.0, 1025.0, 589.0, 293.0, 584.0], "score": 0.97, "text": " is similar to that of single-head attention with full dimensionality."}, {"category_id": 15, "poly": [591.0, 525.0, 1404.0, 525.0, 1404.0, 557.0, 591.0, 557.0], "score": 0.99, "text": ". Due to the reduced dimension of each head, the total computational cost"}, {"category_id": 15, "poly": [296.0, 493.0, 590.0, 493.0, 590.0, 525.0, 296.0, 525.0], "score": 0.93, "text": "In this work we employ"}, {"category_id": 15, "poly": [678.0, 493.0, 1402.0, 493.0, 1402.0, 525.0, 678.0, 525.0], "score": 0.98, "text": " parallel attention layers, or heads. For each of these we use"}, {"category_id": 15, "poly": [365.0, 720.0, 1407.0, 722.0, 1407.0, 761.0, 365.0, 759.0], "score": 0.97, "text": "\u25cf In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,"}, {"category_id": 15, "poly": [395.0, 756.0, 1402.0, 756.0, 1402.0, 788.0, 395.0, 788.0], "score": 0.99, "text": "and the memory keys and values come from the output of the encoder. This allows every"}, {"category_id": 15, "poly": [397.0, 788.0, 1404.0, 788.0, 1404.0, 820.0, 397.0, 820.0], "score": 0.99, "text": "position in the decoder to attend over all positions in the input sequence. This mimics the"}, {"category_id": 15, "poly": [397.0, 818.0, 1404.0, 818.0, 1404.0, 850.0, 397.0, 850.0], "score": 1.0, "text": "typical encoder-decoder attention mechanisms in sequence-to-sequence models such as"}, {"category_id": 15, "poly": [397.0, 846.0, 510.0, 846.0, 510.0, 878.0, 397.0, 878.0], "score": 0.97, "text": "[31, 2, 8]."}, {"category_id": 15, "poly": [367.0, 889.0, 1400.0, 889.0, 1400.0, 921.0, 367.0, 921.0], "score": 0.98, "text": "\u25cf The encoder contains self-attention layers. In a self-attention layer all of the keys, values"}, {"category_id": 15, "poly": [395.0, 921.0, 1402.0, 921.0, 1402.0, 951.0, 395.0, 951.0], "score": 0.99, "text": "and queries come from the same place, in this case, the output of the previous layer in the"}, {"category_id": 15, "poly": [397.0, 951.0, 1404.0, 951.0, 1404.0, 981.0, 397.0, 981.0], "score": 0.99, "text": "encoder. Each position in the encoder can attend to all positions in the previous layer of the"}, {"category_id": 15, "poly": [397.0, 983.0, 494.0, 983.0, 494.0, 1008.0, 397.0, 1008.0], "score": 1.0, "text": "encoder."}, {"category_id": 15, "poly": [370.0, 1020.0, 1402.0, 1020.0, 1402.0, 1052.0, 370.0, 1052.0], "score": 0.99, "text": "\u00b7 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to"}, {"category_id": 15, "poly": [397.0, 1052.0, 1402.0, 1052.0, 1402.0, 1084.0, 397.0, 1084.0], "score": 0.99, "text": "all positions in the decoder up to and including that position. We need to prevent leftward"}, {"category_id": 15, "poly": [397.0, 1082.0, 1402.0, 1082.0, 1402.0, 1114.0, 397.0, 1114.0], "score": 0.99, "text": "information fow in the decoder to preserve the auto-regressive property. We implement this"}, {"category_id": 15, "poly": [395.0, 1144.0, 1171.0, 1144.0, 1171.0, 1176.0, 395.0, 1176.0], "score": 1.0, "text": "of the softmax which correspond to illegal connections. See Figure 2."}, {"category_id": 15, "poly": [397.0, 1114.0, 1099.0, 1114.0, 1099.0, 1144.0, 397.0, 1144.0], "score": 0.98, "text": "inside of scaled dot-product attention by masking out (setting to"}, {"category_id": 15, "poly": [1158.0, 1114.0, 1407.0, 1114.0, 1407.0, 1144.0, 1158.0, 1144.0], "score": 0.97, "text": " all values in the input"}, {"category_id": 15, "poly": [298.0, 1265.0, 1400.0, 1265.0, 1400.0, 1297.0, 298.0, 1297.0], "score": 0.99, "text": "In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully"}, {"category_id": 15, "poly": [298.0, 1297.0, 1402.0, 1297.0, 1402.0, 1329.0, 298.0, 1329.0], "score": 0.99, "text": "connected feed-forward network, which is applied to each position separately and identically. This"}, {"category_id": 15, "poly": [296.0, 1325.0, 1113.0, 1327.0, 1113.0, 1359.0, 296.0, 1357.0], "score": 0.99, "text": "consists of two linear transformations with a ReLU activation in between."}, {"category_id": 15, "poly": [293.0, 1613.0, 670.0, 1613.0, 670.0, 1652.0, 293.0, 1652.0], "score": 0.97, "text": " 3.4  Embeddings and Softmax"}, {"category_id": 15, "poly": [291.0, 1856.0, 601.0, 1859.0, 600.0, 1898.0, 291.0, 1895.0], "score": 0.93, "text": " 3.5  Positional Encoding"}, {"category_id": 15, "poly": [296.0, 1914.0, 1402.0, 1916.0, 1402.0, 1948.0, 296.0, 1946.0], "score": 0.99, "text": "Since our model contains no recurrence and no convolution, in order for the model to make use of the"}, {"category_id": 15, "poly": [298.0, 1950.0, 1402.0, 1950.0, 1402.0, 1980.0, 298.0, 1980.0], "score": 1.0, "text": "order of the sequence, we must inject some information about the relative or absolute position of the"}, {"category_id": 15, "poly": [296.0, 1980.0, 1400.0, 1980.0, 1400.0, 2012.0, 296.0, 2012.0], "score": 1.0, "text": "tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the"}, {"category_id": 15, "poly": [296.0, 1455.0, 1402.0, 1458.0, 1402.0, 1490.0, 296.0, 1487.0], "score": 0.99, "text": "While the linear transformations are the same across different positions, they use different parameters"}, {"category_id": 15, "poly": [296.0, 1492.0, 1402.0, 1492.0, 1402.0, 1522.0, 296.0, 1522.0], "score": 0.99, "text": "from layer to layer. Another way of describing this is as two convolutions with kernel size 1."}, {"category_id": 15, "poly": [291.0, 1515.0, 787.0, 1517.0, 787.0, 1556.0, 291.0, 1554.0], "score": 0.99, "text": "The dimensionality of input and output is"}, {"category_id": 15, "poly": [944.0, 1515.0, 1404.0, 1517.0, 1404.0, 1556.0, 944.0, 1554.0], "score": 0.98, "text": ", and the inner-layer has dimensionality"}, {"category_id": 15, "poly": [298.0, 1210.0, 815.0, 1210.0, 815.0, 1242.0, 298.0, 1242.0], "score": 1.0, "text": "3.3Position-wise Feed-Forward Networks"}, {"category_id": 15, "poly": [298.0, 1673.0, 1404.0, 1673.0, 1404.0, 1705.0, 298.0, 1705.0], "score": 0.99, "text": "Similarly to other sequence transduction models, we use learned embeddings to convert the input"}, {"category_id": 15, "poly": [298.0, 1732.0, 1404.0, 1732.0, 1404.0, 1765.0, 298.0, 1765.0], "score": 0.99, "text": "mation and softmax function to convert the decoder output to predicted next-token probabilities. In"}, {"category_id": 15, "poly": [298.0, 1765.0, 1402.0, 1765.0, 1402.0, 1797.0, 298.0, 1797.0], "score": 0.97, "text": "our model, we share the same weight matrix between the two embedding layers and the pre-softmax"}, {"category_id": 15, "poly": [296.0, 1794.0, 1308.0, 1794.0, 1308.0, 1826.0, 296.0, 1826.0], "score": 0.99, "text": "linear transformation, similar to [24]. In the embedding layers, we multiply those weights by"}, {"category_id": 15, "poly": [296.0, 1703.0, 835.0, 1703.0, 835.0, 1735.0, 296.0, 1735.0], "score": 0.98, "text": "tokens and output tokens to vectors of dimension"}, {"category_id": 15, "poly": [904.0, 1703.0, 1402.0, 1703.0, 1402.0, 1735.0, 904.0, 1735.0], "score": 0.97, "text": ". We also use the usual learned linear transfor-"}, {"category_id": 15, "poly": [293.0, 442.0, 344.0, 437.0, 344.0, 477.0, 294.0, 481.0], "score": 1.0, "text": "and"}, {"category_id": 15, "poly": [293.0, 408.0, 791.0, 403.0, 791.0, 442.0, 293.0, 447.0], "score": 1.0, "text": "Where the projections are parameter matrices"}, {"category_id": 15, "poly": [1194.0, 408.0, 1397.0, 403.0, 1397.0, 442.0, 1194.0, 447.0], "score": 0.75, "text": ", WV E IRdmodel Xdu"}, {"category_id": 15, "poly": [293.0, 665.0, 1037.0, 667.0, 1037.0, 706.0, 293.0, 704.0], "score": 0.97, "text": " The Transformer uses multi-head attention in three different ways:"}, {"category_id": 15, "poly": [845.0, 2069.0, 859.0, 2069.0, 859.0, 2081.0, 845.0, 2081.0], "score": 0.95, "text": "5"}], "page_info": {"page_no": 4, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 1, "poly": [294.9167175292969, 1792.34423828125, 1405.74462890625, 1792.34423828125, 1405.74462890625, 2010.4539794921875, 294.9167175292969, 2010.4539794921875], "score": 0.9999930262565613}, {"category_id": 1, "poly": [296.0656433105469, 1564.883544921875, 1404.7274169921875, 1564.883544921875, 1404.7274169921875, 1781.838134765625, 296.0656433105469, 1781.838134765625], "score": 0.9999913573265076}, {"category_id": 0, "poly": [296.1640319824219, 1250.534912109375, 630.9632568359375, 1250.534912109375, 630.9632568359375, 1291.746826171875, 296.1640319824219, 1291.746826171875], "score": 0.9999879598617554}, {"category_id": 1, "poly": [296.1152038574219, 914.268310546875, 1405.0218505859375, 914.268310546875, 1405.0218505859375, 1072.15283203125, 296.1152038574219, 1072.15283203125], "score": 0.9999794960021973}, {"category_id": 1, "poly": [297.49078369140625, 1321.957275390625, 1405.5133056640625, 1321.957275390625, 1405.5133056640625, 1476.2384033203125, 297.49078369140625, 1476.2384033203125], "score": 0.9999788999557495}, {"category_id": 1, "poly": [296.82354736328125, 1082.3338623046875, 1403.691650390625, 1082.3338623046875, 1403.691650390625, 1207.0330810546875, 296.82354736328125, 1207.0330810546875], "score": 0.9999618530273438}, {"category_id": 1, "poly": [296.37811279296875, 595.3700561523438, 1404.4215087890625, 595.3700561523438, 1404.4215087890625, 688.8590087890625, 296.37811279296875, 688.8590087890625], "score": 0.9999420046806335}, {"category_id": 5, "poly": [325.32757568359375, 307.39581298828125, 1375.2154541015625, 307.39581298828125, 1375.2154541015625, 525.0505981445312, 325.32757568359375, 525.0505981445312], "score": 0.9999203681945801}, {"category_id": 8, "poly": [619.5894165039062, 794.314697265625, 1083.9447021484375, 794.314697265625, 1083.9447021484375, 885.1447143554688, 619.5894165039062, 885.1447143554688], "score": 0.9998595118522644}, {"category_id": 1, "poly": [296.6163024902344, 1489.6407470703125, 1403.7734375, 1489.6407470703125, 1403.7734375, 1554.1478271484375, 296.6163024902344, 1554.1478271484375], "score": 0.9997600317001343}, {"category_id": 1, "poly": [296.73919677734375, 701.4323120117188, 1084.9908447265625, 701.4323120117188, 1084.9908447265625, 734.3526611328125, 296.73919677734375, 734.3526611328125], "score": 0.999642550945282}, {"category_id": 2, "poly": [840.0984497070312, 2060.120849609375, 859.7106323242188, 2060.120849609375, 859.7106323242188, 2087.0458984375, 840.0984497070312, 2087.0458984375], "score": 0.9994811415672302}, {"category_id": 6, "poly": [294.99835205078125, 193.1616668701172, 1405.1517333984375, 193.1616668701172, 1405.1517333984375, 288.5198974609375, 294.99835205078125, 288.5198974609375], "score": 0.9870066046714783}, {"category_id": 1, "poly": [295.5603332519531, 193.08522033691406, 1405.11376953125, 193.08522033691406, 1405.11376953125, 288.22784423828125, 295.5603332519531, 288.22784423828125], "score": 0.28924739360809326}, {"category_id": 13, "poly": [296, 1385, 425, 1385, 425, 1418, 296, 1418], "score": 0.94, "latex": "(x_{1},...,x_{n})"}, {"category_id": 13, "poly": [939, 1824, 1001, 1824, 1001, 1857, 939, 1857], "score": 0.93, "latex": "O(n)"}, {"category_id": 13, "poly": [855, 1385, 978, 1385, 978, 1418, 855, 1418], "score": 0.92, "latex": "\\left(z_{1},...,z_{n}\\right)"}, {"category_id": 14, "poly": [623, 790, 1078, 790, 1078, 888, 623, 888], "score": 0.91, "latex": "\\begin{array}{r}{P E_{(p o s,2i)}=s i n(p o s/10000^{2i/d_{\\mathrm{model}}})}\\\\ {P E_{(p o s,2i+1)}=c o s(p o s/10000^{2i/d_{\\mathrm{model}}})}\\end{array}"}, {"category_id": 13, "poly": [1052, 1383, 1190, 1383, 1190, 1416, 1052, 1416], "score": 0.91, "latex": "x_{i},z_{i}\\in\\dot{\\mathbb{R}^{d}}"}, {"category_id": 13, "poly": [820, 1008, 929, 1008, 929, 1041, 820, 1041], "score": 0.9, "latex": "P E_{p o s+k}"}, {"category_id": 13, "poly": [298, 1038, 376, 1038, 376, 1072, 298, 1072], "score": 0.89, "latex": "P E_{p o s}"}, {"category_id": 13, "poly": [1229, 947, 1351, 947, 1351, 976, 1229, 976], "score": 0.89, "latex": "10000\\cdot2\\pi"}, {"category_id": 13, "poly": [1333, 596, 1401, 596, 1401, 627, 1333, 627], "score": 0.87, "latex": "d_{\\mathrm{model}}"}, {"category_id": 13, "poly": [1164, 949, 1198, 949, 1198, 975, 1164, 975], "score": 0.82, "latex": "2\\pi"}, {"category_id": 13, "poly": [627, 919, 640, 919, 640, 944, 627, 944], "score": 0.81, "latex": "i"}, {"category_id": 13, "poly": [1239, 226, 1258, 226, 1258, 252, 1239, 252], "score": 0.79, "latex": "k"}, {"category_id": 13, "poly": [858, 226, 876, 226, 876, 252, 858, 252], "score": 0.78, "latex": "d"}, {"category_id": 13, "poly": [975, 1886, 992, 1886, 992, 1912, 975, 1912], "score": 0.76, "latex": "d"}, {"category_id": 13, "poly": [377, 1890, 398, 1890, 398, 1912, 377, 1912], "score": 0.74, "latex": "n"}, {"category_id": 13, "poly": [573, 261, 589, 261, 589, 282, 573, 282], "score": 0.73, "latex": "r"}, {"category_id": 13, "poly": [1356, 1981, 1373, 1981, 1373, 2003, 1356, 2003], "score": 0.7, "latex": "r"}, {"category_id": 13, "poly": [576, 231, 596, 231, 596, 252, 576, 252], "score": 0.66, "latex": "n"}, {"category_id": 13, "poly": [733, 386, 841, 386, 841, 418, 733, 418], "score": 0.59, "latex": "O(n^{2}\\cdot d)"}, {"category_id": 13, "poly": [791, 1009, 809, 1009, 809, 1035, 791, 1035], "score": 0.56, "latex": "k"}, {"category_id": 15, "poly": [296.0, 1790.0, 1402.0, 1792.0, 1402.0, 1831.0, 296.0, 1829.0], "score": 0.98, "text": "As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially"}, {"category_id": 15, "poly": [298.0, 1856.0, 1402.0, 1856.0, 1402.0, 1888.0, 298.0, 1888.0], "score": 1.0, "text": "computational complexity, self-attention layers are faster than recurrent layers when the sequence"}, {"category_id": 15, "poly": [293.0, 1914.0, 1407.0, 1911.0, 1407.0, 1950.0, 293.0, 1953.0], "score": 0.99, "text": " sentence representations used by state-of-the-art models in machine translations, such as word-piece"}, {"category_id": 15, "poly": [298.0, 1948.0, 1402.0, 1948.0, 1402.0, 1980.0, 298.0, 1980.0], "score": 0.99, "text": "[31] and byte-pair [25] representations. To improve computational performance for tasks involving"}, {"category_id": 15, "poly": [296.0, 1826.0, 938.0, 1826.0, 938.0, 1859.0, 296.0, 1859.0], "score": 0.98, "text": "executed operations, whereas a recurrent layer requires"}, {"category_id": 15, "poly": [1002.0, 1826.0, 1400.0, 1826.0, 1400.0, 1859.0, 1002.0, 1859.0], "score": 0.97, "text": " sequential operations. In terms of"}, {"category_id": 15, "poly": [993.0, 1888.0, 1402.0, 1888.0, 1402.0, 1918.0, 993.0, 1918.0], "score": 0.98, "text": ", which is most often the case with"}, {"category_id": 15, "poly": [296.0, 1888.0, 376.0, 1888.0, 376.0, 1918.0, 296.0, 1918.0], "score": 1.0, "text": "length"}, {"category_id": 15, "poly": [399.0, 1888.0, 974.0, 1888.0, 974.0, 1918.0, 399.0, 1918.0], "score": 0.98, "text": " is smaller than the representation dimensionality"}, {"category_id": 15, "poly": [296.0, 1980.0, 1355.0, 1980.0, 1355.0, 2012.0, 296.0, 2012.0], "score": 0.99, "text": "very long sequences, self-attention could be restricted to considering only a neighborhood of size"}, {"category_id": 15, "poly": [1374.0, 1980.0, 1402.0, 1980.0, 1402.0, 2012.0, 1374.0, 2012.0], "score": 0.99, "text": "in"}, {"category_id": 15, "poly": [293.0, 1563.0, 1404.0, 1565.0, 1404.0, 1604.0, 293.0, 1602.0], "score": 0.99, "text": "The third is the path length between long-range dependencies in the network. Learning long-range"}, {"category_id": 15, "poly": [298.0, 1600.0, 1402.0, 1600.0, 1402.0, 1632.0, 298.0, 1632.0], "score": 0.99, "text": "dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the"}, {"category_id": 15, "poly": [298.0, 1629.0, 1404.0, 1629.0, 1404.0, 1659.0, 298.0, 1659.0], "score": 0.99, "text": "ability to learn such dependencies is the length of the paths forward and backward signals have to"}, {"category_id": 15, "poly": [296.0, 1661.0, 1404.0, 1661.0, 1404.0, 1691.0, 296.0, 1691.0], "score": 0.99, "text": "traverse in the network. The shorter these paths between any combination of positions in the input"}, {"category_id": 15, "poly": [296.0, 1691.0, 1402.0, 1691.0, 1402.0, 1723.0, 296.0, 1723.0], "score": 0.99, "text": "and output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare"}, {"category_id": 15, "poly": [296.0, 1721.0, 1402.0, 1721.0, 1402.0, 1753.0, 296.0, 1753.0], "score": 0.99, "text": "the maximum path length between any two input and output positions in networks composed of the"}, {"category_id": 15, "poly": [296.0, 1748.0, 529.0, 1753.0, 528.0, 1785.0, 295.0, 1780.0], "score": 0.95, "text": "different layer types."}, {"category_id": 15, "poly": [291.0, 1251.0, 631.0, 1251.0, 631.0, 1297.0, 291.0, 1297.0], "score": 0.98, "text": "4  Why Self-Attention"}, {"category_id": 15, "poly": [298.0, 981.0, 1402.0, 981.0, 1402.0, 1011.0, 298.0, 1011.0], "score": 0.99, "text": "chose this function because we hypothesized it would allow the model to easily learn to attend by"}, {"category_id": 15, "poly": [930.0, 1008.0, 1404.0, 1008.0, 1404.0, 1047.0, 930.0, 1047.0], "score": 0.99, "text": "can be represented as a linear function of"}, {"category_id": 15, "poly": [1352.0, 949.0, 1402.0, 946.0, 1402.0, 979.0, 1352.0, 981.0], "score": 0.99, "text": "We"}, {"category_id": 15, "poly": [296.0, 949.0, 1163.0, 946.0, 1163.0, 979.0, 296.0, 981.0], "score": 0.99, "text": "corresponds to a sinusoid. The wavelengths form a geometric progression from"}, {"category_id": 15, "poly": [1199.0, 949.0, 1228.0, 946.0, 1228.0, 979.0, 1199.0, 981.0], "score": 1.0, "text": "to"}, {"category_id": 15, "poly": [298.0, 919.0, 626.0, 919.0, 626.0, 951.0, 298.0, 951.0], "score": 1.0, "text": "where pos is the position and"}, {"category_id": 15, "poly": [641.0, 919.0, 1402.0, 919.0, 1402.0, 951.0, 641.0, 951.0], "score": 1.0, "text": "is the dimension. That is, each dimension of the positional encoding"}, {"category_id": 15, "poly": [296.0, 1008.0, 790.0, 1008.0, 790.0, 1047.0, 296.0, 1047.0], "score": 0.98, "text": "relative positions, since for any fixed offset"}, {"category_id": 15, "poly": [810.0, 1008.0, 819.0, 1008.0, 819.0, 1047.0, 810.0, 1047.0], "score": 0.89, "text": "\uff0c"}, {"category_id": 15, "poly": [296.0, 1327.0, 1402.0, 1327.0, 1402.0, 1359.0, 296.0, 1359.0], "score": 0.98, "text": "In this section we compare various aspects of self-attention layers to the recurrent and convolu-"}, {"category_id": 15, "poly": [293.0, 1352.0, 1404.0, 1354.0, 1404.0, 1393.0, 293.0, 1391.0], "score": 1.0, "text": "tional layers commonly used for mapping one variable-length sequence of symbol representations"}, {"category_id": 15, "poly": [296.0, 1419.0, 1402.0, 1419.0, 1402.0, 1451.0, 296.0, 1451.0], "score": 0.99, "text": "layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we"}, {"category_id": 15, "poly": [293.0, 1446.0, 582.0, 1446.0, 582.0, 1476.0, 293.0, 1476.0], "score": 0.97, "text": "consider three desiderata."}, {"category_id": 15, "poly": [426.0, 1384.0, 854.0, 1382.0, 854.0, 1421.0, 426.0, 1423.0], "score": 0.97, "text": "to another sequence of equal length"}, {"category_id": 15, "poly": [979.0, 1384.0, 1051.0, 1382.0, 1051.0, 1421.0, 979.0, 1423.0], "score": 0.85, "text": ",with"}, {"category_id": 15, "poly": [1191.0, 1384.0, 1407.0, 1382.0, 1407.0, 1421.0, 1191.0, 1423.0], "score": 0.96, "text": ", such as a hidden"}, {"category_id": 15, "poly": [296.0, 1086.0, 1404.0, 1086.0, 1404.0, 1118.0, 296.0, 1118.0], "score": 1.0, "text": "We also experimented with using learned positional embeddings [8] instead, and found that the two"}, {"category_id": 15, "poly": [298.0, 1116.0, 1402.0, 1116.0, 1402.0, 1148.0, 298.0, 1148.0], "score": 0.99, "text": "versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version"}, {"category_id": 15, "poly": [296.0, 1148.0, 1400.0, 1148.0, 1400.0, 1178.0, 296.0, 1178.0], "score": 0.99, "text": "because it may allow the model to extrapolate to sequence lengths longer than the ones encountered"}, {"category_id": 15, "poly": [294.0, 1175.0, 471.0, 1178.0, 471.0, 1210.0, 293.0, 1208.0], "score": 0.95, "text": "during training."}, {"category_id": 15, "poly": [293.0, 623.0, 1404.0, 628.0, 1404.0, 667.0, 293.0, 662.0], "score": 0.98, "text": " as the embeddings, so that the two can be summed. There are many choices of positional encodings,"}, {"category_id": 15, "poly": [298.0, 660.0, 538.0, 660.0, 538.0, 692.0, 298.0, 692.0], "score": 0.97, "text": "learned and fixed [8]."}, {"category_id": 15, "poly": [293.0, 591.0, 1332.0, 594.0, 1332.0, 633.0, 293.0, 630.0], "score": 0.99, "text": " bottoms of the encoder and decoder stacks. The positional encodings have the same dimension"}, {"category_id": 15, "poly": [298.0, 1494.0, 1402.0, 1494.0, 1402.0, 1526.0, 298.0, 1526.0], "score": 1.0, "text": "One is the total computational complexity per layer. Another is the amount of computation that can"}, {"category_id": 15, "poly": [298.0, 1524.0, 1270.0, 1524.0, 1270.0, 1556.0, 298.0, 1556.0], "score": 0.99, "text": "be parallelized, as measured by the minimum number of sequential operations required."}, {"category_id": 15, "poly": [298.0, 704.0, 1083.0, 704.0, 1083.0, 736.0, 298.0, 736.0], "score": 0.99, "text": "In this work, we use sine and cosine functions of different frequencies:"}, {"category_id": 15, "poly": [845.0, 2069.0, 859.0, 2069.0, 859.0, 2081.0, 845.0, 2081.0], "score": 0.58, "text": "b"}, {"category_id": 15, "poly": [298.0, 197.0, 1402.0, 197.0, 1402.0, 229.0, 298.0, 229.0], "score": 0.99, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations"}, {"category_id": 15, "poly": [1259.0, 227.0, 1402.0, 227.0, 1402.0, 259.0, 1259.0, 259.0], "score": 0.95, "text": "is the kernel"}, {"category_id": 15, "poly": [877.0, 227.0, 1238.0, 227.0, 1238.0, 259.0, 877.0, 259.0], "score": 1.0, "text": "is the representation dimension,"}, {"category_id": 15, "poly": [296.0, 259.0, 572.0, 259.0, 572.0, 289.0, 296.0, 289.0], "score": 0.98, "text": "size of convolutions and"}, {"category_id": 15, "poly": [590.0, 259.0, 1210.0, 259.0, 1210.0, 289.0, 590.0, 289.0], "score": 1.0, "text": "the size of the neighborhood in restricted self-attention."}, {"category_id": 15, "poly": [298.0, 227.0, 575.0, 227.0, 575.0, 259.0, 298.0, 259.0], "score": 0.99, "text": "for different layer types."}, {"category_id": 15, "poly": [597.0, 227.0, 857.0, 227.0, 857.0, 259.0, 597.0, 259.0], "score": 1.0, "text": "is the sequence length,"}, {"category_id": 15, "poly": [298.0, 197.0, 1402.0, 197.0, 1402.0, 229.0, 298.0, 229.0], "score": 0.99, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations"}, {"category_id": 15, "poly": [1259.0, 227.0, 1402.0, 227.0, 1402.0, 259.0, 1259.0, 259.0], "score": 0.95, "text": "is the kernel"}, {"category_id": 15, "poly": [877.0, 227.0, 1238.0, 227.0, 1238.0, 259.0, 877.0, 259.0], "score": 1.0, "text": "is the representation dimension,"}, {"category_id": 15, "poly": [296.0, 259.0, 572.0, 259.0, 572.0, 289.0, 296.0, 289.0], "score": 0.98, "text": "size of convolutions and"}, {"category_id": 15, "poly": [590.0, 259.0, 1210.0, 259.0, 1210.0, 289.0, 590.0, 289.0], "score": 1.0, "text": "the size of the neighborhood in restricted self-attention."}, {"category_id": 15, "poly": [298.0, 227.0, 575.0, 227.0, 575.0, 259.0, 298.0, 259.0], "score": 0.99, "text": "for different layer types."}, {"category_id": 15, "poly": [597.0, 227.0, 857.0, 227.0, 857.0, 259.0, 597.0, 259.0], "score": 1.0, "text": "is the sequence length,"}], "page_info": {"page_no": 5, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 0, "poly": [299.3993835449219, 1760.545654296875, 542.73681640625, 1760.545654296875, 542.73681640625, 1794.2001953125, 299.3993835449219, 1794.2001953125], "score": 0.9999963641166687}, {"category_id": 8, "poly": [449.8925476074219, 1564.28564453125, 1252.9879150390625, 1564.28564453125, 1252.9879150390625, 1604.8138427734375, 449.8925476074219, 1604.8138427734375], "score": 0.9999949336051941}, {"category_id": 0, "poly": [297.4052429199219, 846.1103515625, 696.5767822265625, 846.1103515625, 696.5767822265625, 877.5036010742188, 297.4052429199219, 877.5036010742188], "score": 0.9999938011169434}, {"category_id": 0, "poly": [297.8648376464844, 704.4945068359375, 479.7108459472656, 704.4945068359375, 479.7108459472656, 743.0578002929688, 297.8648376464844, 743.0578002929688], "score": 0.999991774559021}, {"category_id": 1, "poly": [299.64990234375, 1629.92724609375, 1404.3070068359375, 1629.92724609375, 1404.3070068359375, 1722.9583740234375, 299.64990234375, 1722.9583740234375], "score": 0.9999890327453613}, {"category_id": 1, "poly": [297.67523193359375, 279.1004638671875, 1404.9674072265625, 279.1004638671875, 1404.9674072265625, 523.8114624023438, 297.67523193359375, 523.8114624023438], "score": 0.9999877214431763}, {"category_id": 1, "poly": [299.2789001464844, 536.8982543945312, 1403.3575439453125, 536.8982543945312, 1403.3575439453125, 658.7787475585938, 299.2789001464844, 658.7787475585938], "score": 0.9999865293502808}, {"category_id": 9, "poly": [1367.7454833984375, 1572.0401611328125, 1399.8514404296875, 1572.0401611328125, 1399.8514404296875, 1599.91748046875, 1367.7454833984375, 1599.91748046875], "score": 0.9999861717224121}, {"category_id": 1, "poly": [298.5527648925781, 1883.8248291015625, 1404.0946044921875, 1883.8248291015625, 1404.0946044921875, 2008.7952880859375, 298.5527648925781, 2008.7952880859375], "score": 0.9999825358390808}, {"category_id": 1, "poly": [298.5526428222656, 901.6740112304688, 1405.3323974609375, 901.6740112304688, 1405.3323974609375, 1115.079345703125, 298.5526428222656, 1115.079345703125], "score": 0.9999796748161316}, {"category_id": 0, "poly": [298.23382568359375, 1402.133056640625, 487.3485412597656, 1402.133056640625, 487.3485412597656, 1435.9560546875, 298.23382568359375, 1435.9560546875], "score": 0.9999794960021973}, {"category_id": 1, "poly": [297.96063232421875, 1210.709716796875, 1404.5032958984375, 1210.709716796875, 1404.5032958984375, 1363.7554931640625, 297.96063232421875, 1363.7554931640625], "score": 0.999976634979248}, {"category_id": 0, "poly": [296.7562255859375, 1154.3751220703125, 651.16943359375, 1154.3751220703125, 651.16943359375, 1186.05029296875, 296.7562255859375, 1186.05029296875], "score": 0.9999627470970154}, {"category_id": 1, "poly": [298.0361022949219, 1457.87744140625, 1402.870361328125, 1457.87744140625, 1402.870361328125, 1523.4527587890625, 298.0361022949219, 1523.4527587890625], "score": 0.9999369382858276}, {"category_id": 1, "poly": [299.3161315917969, 1816.8477783203125, 924.413818359375, 1816.8477783203125, 924.413818359375, 1850.4219970703125, 299.3161315917969, 1850.4219970703125], "score": 0.999841570854187}, {"category_id": 1, "poly": [298.99188232421875, 204.4054718017578, 1403.342529296875, 204.4054718017578, 1403.342529296875, 265.876708984375, 298.99188232421875, 265.876708984375], "score": 0.9994466304779053}, {"category_id": 1, "poly": [299.7109069824219, 776.95947265625, 938.5873413085938, 776.95947265625, 938.5873413085938, 806.959228515625, 299.7109069824219, 806.959228515625], "score": 0.9994182586669922}, {"category_id": 2, "poly": [842.7471923828125, 2061.54541015625, 858.6443481445312, 2061.54541015625, 858.6443481445312, 2083.491455078125, 842.7471923828125, 2083.491455078125], "score": 0.9980531930923462}, {"category_id": 13, "poly": [720, 309, 811, 309, 811, 343, 720, 343], "score": 0.94, "latex": "O(n/k)"}, {"category_id": 13, "poly": [330, 339, 462, 339, 462, 373, 330, 373], "score": 0.93, "latex": "O(l o g_{k}(n))"}, {"category_id": 13, "poly": [457, 233, 545, 233, 545, 267, 457, 267], "score": 0.93, "latex": "O(n/r)"}, {"category_id": 13, "poly": [486, 430, 722, 430, 722, 464, 486, 464], "score": 0.92, "latex": "\\dot{O(k\\cdot n\\cdot d+n\\cdot d^{2})}"}, {"category_id": 13, "poly": [863, 431, 939, 431, 939, 459, 863, 459], "score": 0.91, "latex": "k=n"}, {"category_id": 13, "poly": [297, 1976, 437, 1976, 437, 2010, 297, 2010], "score": 0.9, "latex": "P_{d r o p}=0.1"}, {"category_id": 13, "poly": [1023, 1458, 1130, 1458, 1130, 1488, 1023, 1488], "score": 0.9, "latex": "\\epsilon=10^{-9}"}, {"category_id": 13, "poly": [820, 280, 893, 280, 893, 308, 820, 308], "score": 0.9, "latex": "k<n"}, {"category_id": 13, "poly": [740, 1460, 843, 1460, 843, 1491, 740, 1491], "score": 0.88, "latex": "\\beta_{1}=0.9"}, {"category_id": 14, "poly": [447, 1562, 1252, 1562, 1252, 1606, 447, 1606], "score": 0.87, "latex": "l r a t e=d_{\\mathrm{model}}^{-0.5}\\cdot\\mathrm{min}(s t e p\\_n u m^{-0.5},s t e p\\_n u m\\cdot w a r m u p\\_s t e p s^{-1.5})"}, {"category_id": 13, "poly": [854, 1460, 972, 1460, 972, 1491, 854, 1491], "score": 0.84, "latex": "\\beta_{2}=0.98"}, {"category_id": 13, "poly": [656, 402, 674, 402, 674, 428, 656, 428], "score": 0.78, "latex": "k"}, {"category_id": 13, "poly": [479, 1692, 571, 1692, 571, 1720, 479, 1720], "score": 0.55, "latex": "=4000"}, {"category_id": 15, "poly": [293.0, 1760.0, 543.0, 1760.0, 543.0, 1799.0, 293.0, 1799.0], "score": 0.99, "text": "5.4  Regularization"}, {"category_id": 15, "poly": [291.0, 843.0, 700.0, 846.0, 700.0, 885.0, 291.0, 882.0], "score": 0.97, "text": " 5.1 Training Data and Batching"}, {"category_id": 15, "poly": [290.0, 701.0, 481.0, 706.0, 480.0, 752.0, 288.0, 747.0], "score": 0.98, "text": "5 Training"}, {"category_id": 15, "poly": [293.0, 1627.0, 1404.0, 1632.0, 1404.0, 1671.0, 293.0, 1666.0], "score": 0.99, "text": "This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,"}, {"category_id": 15, "poly": [296.0, 1661.0, 1404.0, 1661.0, 1404.0, 1700.0, 296.0, 1700.0], "score": 0.99, "text": "and decreasing it thereafter proportionally to the inverse square root of the step number. We used"}, {"category_id": 15, "poly": [293.0, 1691.0, 478.0, 1689.0, 478.0, 1728.0, 294.0, 1730.0], "score": 1.0, "text": "warmup_steps"}, {"category_id": 15, "poly": [291.0, 367.0, 1407.0, 369.0, 1407.0, 408.0, 291.0, 406.0], "score": 0.98, "text": " between any two positions in the network. Convolutional layers are generally more expensive than"}, {"category_id": 15, "poly": [296.0, 463.0, 1407.0, 463.0, 1407.0, 495.0, 296.0, 495.0], "score": 0.99, "text": "convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,"}, {"category_id": 15, "poly": [296.0, 495.0, 691.0, 495.0, 691.0, 527.0, 296.0, 527.0], "score": 1.0, "text": "the approach we take in our model."}, {"category_id": 15, "poly": [291.0, 309.0, 719.0, 307.0, 719.0, 346.0, 291.0, 348.0], "score": 0.98, "text": " positions. Doing so requires a stack of"}, {"category_id": 15, "poly": [812.0, 309.0, 1407.0, 307.0, 1407.0, 346.0, 812.0, 348.0], "score": 0.98, "text": "convolutional layers in the case of contiguous kernels,"}, {"category_id": 15, "poly": [296.0, 341.0, 329.0, 341.0, 329.0, 374.0, 296.0, 374.0], "score": 0.8, "text": "or"}, {"category_id": 15, "poly": [463.0, 341.0, 1402.0, 341.0, 1402.0, 374.0, 463.0, 374.0], "score": 0.99, "text": " in the case of dilated convolutions [15], increasing the length of the longest paths"}, {"category_id": 15, "poly": [296.0, 433.0, 485.0, 433.0, 485.0, 465.0, 296.0, 465.0], "score": 1.0, "text": "considerably, to"}, {"category_id": 15, "poly": [723.0, 433.0, 862.0, 433.0, 862.0, 465.0, 723.0, 465.0], "score": 0.97, "text": ". Even with"}, {"category_id": 15, "poly": [940.0, 433.0, 1404.0, 433.0, 1404.0, 465.0, 940.0, 465.0], "score": 0.99, "text": ", however, the complexity of a separable"}, {"category_id": 15, "poly": [296.0, 282.0, 819.0, 282.0, 819.0, 314.0, 296.0, 314.0], "score": 0.99, "text": "A single convolutional layer with kernel width"}, {"category_id": 15, "poly": [894.0, 282.0, 1402.0, 282.0, 1402.0, 314.0, 894.0, 314.0], "score": 0.99, "text": "does not connect all pairs of input and output"}, {"category_id": 15, "poly": [296.0, 401.0, 655.0, 401.0, 655.0, 433.0, 296.0, 433.0], "score": 0.99, "text": "recurrent layers, by a factor of"}, {"category_id": 15, "poly": [675.0, 401.0, 1400.0, 401.0, 1400.0, 433.0, 675.0, 433.0], "score": 0.99, "text": ". Separable convolutions [6], however, decrease the complexity"}, {"category_id": 15, "poly": [298.0, 539.0, 1402.0, 539.0, 1402.0, 571.0, 298.0, 571.0], "score": 0.99, "text": "As side benefit, self-attention could yield more interpretable models. We inspect attention distributions"}, {"category_id": 15, "poly": [298.0, 571.0, 1402.0, 571.0, 1402.0, 603.0, 298.0, 603.0], "score": 0.99, "text": "from our models and present and discuss examples in the appendix. Not only do individual attention"}, {"category_id": 15, "poly": [298.0, 600.0, 1402.0, 600.0, 1402.0, 632.0, 298.0, 632.0], "score": 0.99, "text": "heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic"}, {"category_id": 15, "poly": [293.0, 630.0, 739.0, 633.0, 739.0, 665.0, 293.0, 662.0], "score": 0.98, "text": " and semantic structure of the sentences."}, {"category_id": 15, "poly": [296.0, 1884.0, 1404.0, 1884.0, 1404.0, 1923.0, 296.0, 1923.0], "score": 0.98, "text": "Residual Dropout  We apply dropout [27] to the output of each sub-layer, before it is added to the"}, {"category_id": 15, "poly": [298.0, 1918.0, 1400.0, 1918.0, 1400.0, 1950.0, 298.0, 1950.0], "score": 0.99, "text": "sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the"}, {"category_id": 15, "poly": [296.0, 1950.0, 1404.0, 1950.0, 1404.0, 1980.0, 296.0, 1980.0], "score": 0.99, "text": "positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of"}, {"category_id": 15, "poly": [293.0, 901.0, 1402.0, 903.0, 1402.0, 935.0, 293.0, 933.0], "score": 0.99, "text": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million"}, {"category_id": 15, "poly": [298.0, 935.0, 1402.0, 935.0, 1402.0, 967.0, 298.0, 967.0], "score": 0.99, "text": "sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-"}, {"category_id": 15, "poly": [298.0, 965.0, 1402.0, 965.0, 1402.0, 997.0, 298.0, 997.0], "score": 0.99, "text": "target vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT"}, {"category_id": 15, "poly": [296.0, 995.0, 1400.0, 995.0, 1400.0, 1027.0, 296.0, 1027.0], "score": 0.99, "text": "2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece"}, {"category_id": 15, "poly": [298.0, 1027.0, 1400.0, 1027.0, 1400.0, 1059.0, 298.0, 1059.0], "score": 0.99, "text": "vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training"}, {"category_id": 15, "poly": [293.0, 1052.0, 1407.0, 1050.0, 1407.0, 1089.0, 293.0, 1091.0], "score": 0.99, "text": "batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000"}, {"category_id": 15, "poly": [296.0, 1089.0, 450.0, 1089.0, 450.0, 1121.0, 296.0, 1121.0], "score": 0.99, "text": "target tokens."}, {"category_id": 15, "poly": [293.0, 1402.0, 487.0, 1402.0, 487.0, 1441.0, 293.0, 1441.0], "score": 1.0, "text": "5.3Optimizer"}, {"category_id": 15, "poly": [296.0, 1210.0, 1402.0, 1215.0, 1402.0, 1247.0, 296.0, 1242.0], "score": 0.99, "text": "We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using"}, {"category_id": 15, "poly": [296.0, 1272.0, 1402.0, 1274.0, 1402.0, 1306.0, 296.0, 1304.0], "score": 0.98, "text": "trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the"}, {"category_id": 15, "poly": [296.0, 1306.0, 1402.0, 1306.0, 1402.0, 1338.0, 296.0, 1338.0], "score": 0.99, "text": "bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps"}, {"category_id": 15, "poly": [296.0, 1336.0, 420.0, 1336.0, 420.0, 1368.0, 296.0, 1368.0], "score": 0.99, "text": "(3.5days)."}, {"category_id": 15, "poly": [293.0, 1155.0, 654.0, 1155.0, 654.0, 1194.0, 293.0, 1194.0], "score": 0.99, "text": "5.2  Hardware and Schedule"}, {"category_id": 15, "poly": [300.0, 1494.0, 935.0, 1494.0, 935.0, 1526.0, 300.0, 1526.0], "score": 0.98, "text": "rate over the course of training, according to the formula:"}, {"category_id": 15, "poly": [1131.0, 1455.0, 1404.0, 1460.0, 1404.0, 1499.0, 1131.0, 1494.0], "score": 0.98, "text": ". We varied the learning"}, {"category_id": 15, "poly": [293.0, 1455.0, 739.0, 1460.0, 739.0, 1499.0, 293.0, 1494.0], "score": 0.99, "text": "We used the Adam optimizer [17] with"}, {"category_id": 15, "poly": [973.0, 1455.0, 1022.0, 1460.0, 1022.0, 1499.0, 973.0, 1494.0], "score": 1.0, "text": "and"}, {"category_id": 15, "poly": [294.0, 1815.0, 922.0, 1820.0, 921.0, 1859.0, 293.0, 1854.0], "score": 1.0, "text": "We employ three types of regularization during training:"}, {"category_id": 15, "poly": [298.0, 206.0, 1404.0, 206.0, 1404.0, 238.0, 298.0, 238.0], "score": 0.98, "text": "the input sequence centered around the respective output position. This would increase the maximum"}, {"category_id": 15, "poly": [298.0, 238.0, 456.0, 238.0, 456.0, 270.0, 298.0, 270.0], "score": 0.97, "text": "pathlength to"}, {"category_id": 15, "poly": [546.0, 238.0, 1215.0, 238.0, 1215.0, 270.0, 546.0, 270.0], "score": 0.99, "text": ". We plan to investigate this approach further in future work."}, {"category_id": 15, "poly": [293.0, 772.0, 938.0, 775.0, 938.0, 814.0, 293.0, 811.0], "score": 0.98, "text": "This section describes the training regime for our models."}], "page_info": {"page_no": 6, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 0, "poly": [297.0849609375, 935.6597290039062, 613.2861328125, 935.6597290039062, 613.2861328125, 969.6368408203125, 297.0849609375, 969.6368408203125], "score": 0.9999942779541016}, {"category_id": 0, "poly": [297.0860290527344, 864.470458984375, 459.9788818359375, 864.470458984375, 459.9788818359375, 902.0692138671875, 297.0860290527344, 902.0692138671875], "score": 0.9999881982803345}, {"category_id": 0, "poly": [298.3970031738281, 1657.871337890625, 571.4378051757812, 1657.871337890625, 571.4378051757812, 1690.0919189453125, 298.3970031738281, 1690.0919189453125], "score": 0.9999781847000122}, {"category_id": 1, "poly": [298.9534912109375, 992.5917358398438, 1405.4827880859375, 992.5917358398438, 1405.4827880859375, 1176.881103515625, 298.9534912109375, 1176.881103515625], "score": 0.9999775886535645}, {"category_id": 5, "poly": [365.3901062011719, 257.7928466796875, 1345.2271728515625, 257.7928466796875, 1345.2271728515625, 682.1713256835938, 365.3901062011719, 682.1713256835938], "score": 0.9999722242355347}, {"category_id": 1, "poly": [297.2451477050781, 1852.970947265625, 1403.4735107421875, 1852.970947265625, 1403.4735107421875, 1946.531005859375, 297.2451477050781, 1946.531005859375], "score": 0.9999632239341736}, {"category_id": 1, "poly": [298.2964172363281, 1326.608642578125, 1404.381103515625, 1326.608642578125, 1404.381103515625, 1481.1685791015625, 298.2964172363281, 1481.1685791015625], "score": 0.9999617338180542}, {"category_id": 1, "poly": [298.5424499511719, 1189.89501953125, 1404.3868408203125, 1189.89501953125, 1404.3868408203125, 1314.0728759765625, 298.5424499511719, 1314.0728759765625], "score": 0.9999583959579468}, {"category_id": 1, "poly": [299.38330078125, 1493.30810546875, 1403.9881591796875, 1493.30810546875, 1403.9881591796875, 1616.193603515625, 299.38330078125, 1616.193603515625], "score": 0.9999510645866394}, {"category_id": 1, "poly": [298.37664794921875, 1715.14697265625, 1404.31982421875, 1715.14697265625, 1404.31982421875, 1838.3619384765625, 298.37664794921875, 1838.3619384765625], "score": 0.9999436140060425}, {"category_id": 2, "poly": [334.19281005859375, 1977.3699951171875, 1262.8768310546875, 1977.3699951171875, 1262.8768310546875, 2007.5133056640625, 334.19281005859375, 2007.5133056640625], "score": 0.9999043345451355}, {"category_id": 2, "poly": [840.6436157226562, 2060.97509765625, 858.7386474609375, 2060.97509765625, 858.7386474609375, 2086.099365234375, 840.6436157226562, 2086.099365234375], "score": 0.9997522830963135}, {"category_id": 6, "poly": [298.0777587890625, 193.6999053955078, 1405.11865234375, 193.6999053955078, 1405.11865234375, 257.5846252441406, 298.0777587890625, 257.5846252441406], "score": 0.9991278648376465}, {"category_id": 1, "poly": [296.36181640625, 752.9116821289062, 1403.6593017578125, 752.9116821289062, 1403.6593017578125, 819.0323486328125, 296.36181640625, 819.0323486328125], "score": 0.9960073828697205}, {"category_id": 13, "poly": [439, 1282, 577, 1282, 577, 1316, 439, 1316], "score": 0.92, "latex": "P_{d r o p}=0.1"}, {"category_id": 13, "poly": [1168, 754, 1278, 754, 1278, 785, 1168, 785], "score": 0.9, "latex": "\\epsilon_{l s}\\,=\\,0.1"}, {"category_id": 13, "poly": [972, 1388, 1068, 1388, 1068, 1418, 972, 1418], "score": 0.89, "latex": "\\alpha=0.6"}, {"category_id": 13, "poly": [574, 1450, 629, 1450, 629, 1478, 574, 1478], "score": 0.83, "latex": "+~50"}, {"category_id": 13, "poly": [486, 510, 508, 510, 508, 531, 486, 531], "score": 0.64, "latex": "^+"}, {"category_id": 13, "poly": [1101, 1221, 1146, 1221, 1146, 1254, 1101, 1254], "score": 0.63, "latex": "1/4"}, {"category_id": 13, "poly": [485, 379, 509, 379, 509, 401, 485, 401], "score": 0.26, "latex": "^+"}, {"category_id": 15, "poly": [296.0, 940.0, 610.0, 940.0, 610.0, 972.0, 296.0, 972.0], "score": 0.98, "text": "6.1  Machine Translation"}, {"category_id": 15, "poly": [293.0, 866.0, 457.0, 866.0, 457.0, 908.0, 293.0, 908.0], "score": 1.0, "text": "6Results"}, {"category_id": 15, "poly": [296.0, 1659.0, 571.0, 1659.0, 571.0, 1698.0, 296.0, 1698.0], "score": 0.99, "text": "6.2 Model Variations"}, {"category_id": 15, "poly": [293.0, 992.0, 1402.0, 995.0, 1402.0, 1027.0, 293.0, 1024.0], "score": 0.98, "text": " On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)"}, {"category_id": 15, "poly": [298.0, 1027.0, 1404.0, 1027.0, 1404.0, 1059.0, 298.0, 1059.0], "score": 0.99, "text": "in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0"}, {"category_id": 15, "poly": [298.0, 1056.0, 1402.0, 1056.0, 1402.0, 1089.0, 298.0, 1089.0], "score": 0.99, "text": "BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is"}, {"category_id": 15, "poly": [296.0, 1086.0, 1402.0, 1086.0, 1402.0, 1118.0, 296.0, 1118.0], "score": 0.98, "text": "listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model"}, {"category_id": 15, "poly": [293.0, 1118.0, 1402.0, 1118.0, 1402.0, 1150.0, 293.0, 1150.0], "score": 1.0, "text": " surpasses all previously published models and ensembles, at a fraction of the training cost of any of"}, {"category_id": 15, "poly": [296.0, 1148.0, 566.0, 1148.0, 566.0, 1180.0, 296.0, 1180.0], "score": 0.99, "text": "the competitive models."}, {"category_id": 15, "poly": [296.0, 1856.0, 1400.0, 1856.0, 1400.0, 1886.0, 296.0, 1886.0], "score": 0.98, "text": "In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions."}, {"category_id": 15, "poly": [296.0, 1886.0, 1402.0, 1884.0, 1402.0, 1916.0, 296.0, 1918.0], "score": 1.0, "text": "keeping the amount of computation constant, as described in Section 3.2.2. While single-head"}, {"category_id": 15, "poly": [293.0, 1916.0, 1333.0, 1916.0, 1333.0, 1948.0, 293.0, 1948.0], "score": 0.98, "text": " attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads."}, {"category_id": 15, "poly": [298.0, 1329.0, 1402.0, 1329.0, 1402.0, 1361.0, 298.0, 1361.0], "score": 0.99, "text": "For the base models, we used a single model obtained by averaging the last 5 checkpoints, which"}, {"category_id": 15, "poly": [296.0, 1359.0, 1402.0, 1359.0, 1402.0, 1391.0, 296.0, 1391.0], "score": 0.99, "text": "were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We"}, {"category_id": 15, "poly": [300.0, 1421.0, 1404.0, 1421.0, 1404.0, 1453.0, 300.0, 1453.0], "score": 0.99, "text": "were chosen after experimentation on the development set. We set the maximum output length during"}, {"category_id": 15, "poly": [296.0, 1386.0, 971.0, 1391.0, 971.0, 1423.0, 296.0, 1419.0], "score": 0.99, "text": "used beam search with a beam size of 4 and length penalty"}, {"category_id": 15, "poly": [1069.0, 1386.0, 1402.0, 1391.0, 1402.0, 1423.0, 1069.0, 1419.0], "score": 0.98, "text": "[31]. These hyperparameters"}, {"category_id": 15, "poly": [296.0, 1453.0, 573.0, 1453.0, 573.0, 1483.0, 296.0, 1483.0], "score": 0.98, "text": "inference to input length"}, {"category_id": 15, "poly": [630.0, 1453.0, 1079.0, 1453.0, 1079.0, 1483.0, 630.0, 1483.0], "score": 0.99, "text": ", but terminate early when possible [31]."}, {"category_id": 15, "poly": [296.0, 1189.0, 1404.0, 1192.0, 1404.0, 1224.0, 296.0, 1221.0], "score": 0.99, "text": "On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,"}, {"category_id": 15, "poly": [298.0, 1256.0, 1402.0, 1256.0, 1402.0, 1288.0, 298.0, 1288.0], "score": 0.99, "text": "previous state-of-the-art model. The Transformer (big) model trained for English-to-French used"}, {"category_id": 15, "poly": [298.0, 1286.0, 438.0, 1286.0, 438.0, 1318.0, 298.0, 1318.0], "score": 1.0, "text": "dropoutrate"}, {"category_id": 15, "poly": [578.0, 1286.0, 751.0, 1286.0, 751.0, 1318.0, 578.0, 1318.0], "score": 0.93, "text": ", instead of 0.3."}, {"category_id": 15, "poly": [298.0, 1224.0, 1100.0, 1224.0, 1100.0, 1256.0, 298.0, 1256.0], "score": 0.99, "text": "outperforming all of the previously published single models, at less than"}, {"category_id": 15, "poly": [1147.0, 1224.0, 1402.0, 1224.0, 1402.0, 1256.0, 1147.0, 1256.0], "score": 0.97, "text": "the training cost of the"}, {"category_id": 15, "poly": [296.0, 1492.0, 1402.0, 1492.0, 1402.0, 1531.0, 296.0, 1531.0], "score": 0.99, "text": "Table 2 summarizes our results and compares our translation quality and training costs to other model"}, {"category_id": 15, "poly": [296.0, 1529.0, 1402.0, 1529.0, 1402.0, 1561.0, 296.0, 1561.0], "score": 0.99, "text": "architectures from the literature. We estimate the number of foating point operations used to train a"}, {"category_id": 15, "poly": [298.0, 1558.0, 1402.0, 1558.0, 1402.0, 1590.0, 298.0, 1590.0], "score": 0.99, "text": "model by multiplying the training time, the number of GPUs used, and an estimate of the sustained"}, {"category_id": 15, "poly": [293.0, 1584.0, 912.0, 1581.0, 912.0, 1620.0, 293.0, 1623.0], "score": 0.98, "text": "single-precision floating-point capacity of each GPU 5."}, {"category_id": 15, "poly": [296.0, 1714.0, 1404.0, 1714.0, 1404.0, 1753.0, 296.0, 1753.0], "score": 0.98, "text": "To evaluate the importance of different components of the Transformer, we varied our base model"}, {"category_id": 15, "poly": [293.0, 1746.0, 1402.0, 1749.0, 1402.0, 1781.0, 293.0, 1778.0], "score": 0.99, "text": " in different ways, measuring the change in performance on English-to-German translation on the"}, {"category_id": 15, "poly": [296.0, 1776.0, 1404.0, 1781.0, 1404.0, 1813.0, 296.0, 1808.0], "score": 0.99, "text": "development set, newstest2013. We used beam search as described in the previous section, but no"}, {"category_id": 15, "poly": [298.0, 1810.0, 942.0, 1810.0, 942.0, 1842.0, 298.0, 1842.0], "score": 0.99, "text": "checkpoint averaging. We present these results in Table 3."}, {"category_id": 15, "poly": [333.0, 1973.0, 1259.0, 1975.0, 1259.0, 2014.0, 333.0, 2012.0], "score": 0.99, "text": "5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively"}, {"category_id": 15, "poly": [845.0, 2069.0, 859.0, 2069.0, 859.0, 2081.0, 845.0, 2081.0], "score": 0.65, "text": "8"}, {"category_id": 15, "poly": [298.0, 197.0, 1402.0, 197.0, 1402.0, 229.0, 298.0, 229.0], "score": 0.99, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the"}, {"category_id": 15, "poly": [296.0, 227.0, 1342.0, 229.0, 1342.0, 261.0, 296.0, 259.0], "score": 1.0, "text": "English-to-German and English-to-French newstest2014 tests at a fraction of the training cost."}, {"category_id": 15, "poly": [296.0, 786.0, 1367.0, 786.0, 1367.0, 818.0, 296.0, 818.0], "score": 0.99, "text": "hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score."}, {"category_id": 15, "poly": [296.0, 756.0, 1167.0, 756.0, 1167.0, 788.0, 296.0, 788.0], "score": 0.98, "text": "Label Smoothing During training, we employed label smoothing of value"}, {"category_id": 15, "poly": [1279.0, 756.0, 1402.0, 756.0, 1402.0, 788.0, 1279.0, 788.0], "score": 0.96, "text": "[30]. This"}], "page_info": {"page_no": 7, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 0, "poly": [297.85455322265625, 1386.6663818359375, 516.865234375, 1386.6663818359375, 516.865234375, 1422.060791015625, 297.85455322265625, 1422.060791015625], "score": 0.9999974966049194}, {"category_id": 5, "poly": [295.8160400390625, 354.4212951660156, 1417.3226318359375, 354.4212951660156, 1417.3226318359375, 1072.36865234375, 295.8160400390625, 1072.36865234375], "score": 0.9999896287918091}, {"category_id": 1, "poly": [298.1799621582031, 1566.9886474609375, 1404.970458984375, 1566.9886474609375, 1404.970458984375, 1690.340576171875, 298.1799621582031, 1690.340576171875], "score": 0.9999570846557617}, {"category_id": 1, "poly": [297.6346435546875, 1702.2392578125, 1405.691650390625, 1702.2392578125, 1405.691650390625, 1827.4910888671875, 297.6346435546875, 1827.4910888671875], "score": 0.9999552965164185}, {"category_id": 1, "poly": [298.08734130859375, 1460.996337890625, 1404.1173095703125, 1460.996337890625, 1404.1173095703125, 1551.845947265625, 298.08734130859375, 1551.845947265625], "score": 0.9999446868896484}, {"category_id": 1, "poly": [298.7926025390625, 1840.4227294921875, 1405.423828125, 1840.4227294921875, 1405.423828125, 1902.03955078125, 298.7926025390625, 1902.03955078125], "score": 0.9998533129692078}, {"category_id": 2, "poly": [839.8329467773438, 2059.93408203125, 859.7399291992188, 2059.93408203125, 859.7399291992188, 2084.896728515625, 839.8329467773438, 2084.896728515625], "score": 0.9993852376937866}, {"category_id": 1, "poly": [297.1435241699219, 1148.268310546875, 1407.413330078125, 1148.268310546875, 1407.413330078125, 1333.5692138671875, 297.1435241699219, 1333.5692138671875], "score": 0.99925297498703}, {"category_id": 1, "poly": [297.820556640625, 1944.556640625, 1406.035888671875, 1944.556640625, 1406.035888671875, 2007.8870849609375, 297.820556640625, 2007.8870849609375], "score": 0.9982642531394958}, {"category_id": 1, "poly": [296.71533203125, 191.9191131591797, 1407.3046875, 191.9191131591797, 1407.3046875, 318.4398498535156, 296.71533203125, 318.4398498535156], "score": 0.8767160177230835}, {"category_id": 6, "poly": [297.04644775390625, 192.4542999267578, 1407.5970458984375, 192.4542999267578, 1407.5970458984375, 318.27508544921875, 297.04644775390625, 318.27508544921875], "score": 0.3167862892150879}, {"category_id": 13, "poly": [1074, 1150, 1105, 1150, 1105, 1180, 1074, 1180], "score": 0.88, "latex": "d_{k}"}, {"category_id": 15, "poly": [296.0, 1386.0, 513.0, 1386.0, 513.0, 1428.0, 296.0, 1428.0], "score": 0.96, "text": "7 Conclusion"}, {"category_id": 15, "poly": [298.0, 1570.0, 1402.0, 1570.0, 1402.0, 1600.0, 298.0, 1600.0], "score": 1.0, "text": "For translation tasks, the Transformer can be trained significantly faster than architectures based"}, {"category_id": 15, "poly": [298.0, 1602.0, 1402.0, 1602.0, 1402.0, 1632.0, 298.0, 1632.0], "score": 0.99, "text": "on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014"}, {"category_id": 15, "poly": [298.0, 1632.0, 1402.0, 1632.0, 1402.0, 1661.0, 298.0, 1661.0], "score": 0.99, "text": "English-to-French translation tasks, we achieve a new state of the art. In the former task our best"}, {"category_id": 15, "poly": [291.0, 1657.0, 956.0, 1659.0, 956.0, 1698.0, 291.0, 1696.0], "score": 0.98, "text": " model outperforms even all previously reported ensembles."}, {"category_id": 15, "poly": [296.0, 1703.0, 1404.0, 1703.0, 1404.0, 1742.0, 296.0, 1742.0], "score": 0.98, "text": "We are excited about the future of attention-based models and plan to apply them to other tasks. We"}, {"category_id": 15, "poly": [298.0, 1737.0, 1402.0, 1737.0, 1402.0, 1769.0, 298.0, 1769.0], "score": 0.99, "text": "plan to extend the Transformer to problems involving input and output modalities other than text and"}, {"category_id": 15, "poly": [296.0, 1769.0, 1402.0, 1769.0, 1402.0, 1799.0, 296.0, 1799.0], "score": 0.99, "text": "to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs"}, {"category_id": 15, "poly": [293.0, 1799.0, 1402.0, 1799.0, 1402.0, 1831.0, 293.0, 1831.0], "score": 0.99, "text": " such as images, audio and video. Making generation less sequential is another research goals of ours."}, {"category_id": 15, "poly": [293.0, 1462.0, 1397.0, 1462.0, 1397.0, 1494.0, 293.0, 1494.0], "score": 1.0, "text": " In this work, we presented the Transformer, the first sequence transduction model based entirely on"}, {"category_id": 15, "poly": [296.0, 1494.0, 1402.0, 1492.0, 1402.0, 1524.0, 296.0, 1526.0], "score": 0.99, "text": "attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with"}, {"category_id": 15, "poly": [296.0, 1526.0, 610.0, 1526.0, 610.0, 1556.0, 296.0, 1556.0], "score": 0.98, "text": "multi-headed self-attention."}, {"category_id": 15, "poly": [296.0, 1840.0, 1400.0, 1843.0, 1400.0, 1875.0, 296.0, 1872.0], "score": 1.0, "text": "The code we used to train and evaluate our models is available at https://github.com/"}, {"category_id": 15, "poly": [298.0, 1875.0, 658.0, 1875.0, 658.0, 1904.0, 298.0, 1904.0], "score": 1.0, "text": "tensorflow/tensor2tensor."}, {"category_id": 15, "poly": [293.0, 1180.0, 1404.0, 1176.0, 1404.0, 1215.0, 293.0, 1219.0], "score": 0.98, "text": "suggests that determining compatibility is not easy and that a more sophisticated compatibility"}, {"category_id": 15, "poly": [296.0, 1215.0, 1402.0, 1215.0, 1402.0, 1244.0, 296.0, 1244.0], "score": 0.98, "text": "function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,"}, {"category_id": 15, "poly": [298.0, 1244.0, 1402.0, 1244.0, 1402.0, 1274.0, 298.0, 1274.0], "score": 0.99, "text": "bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our"}, {"category_id": 15, "poly": [298.0, 1274.0, 1402.0, 1274.0, 1402.0, 1306.0, 298.0, 1306.0], "score": 0.99, "text": "sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical"}, {"category_id": 15, "poly": [296.0, 1304.0, 582.0, 1304.0, 582.0, 1336.0, 296.0, 1336.0], "score": 0.97, "text": "results to the base model."}, {"category_id": 15, "poly": [293.0, 1148.0, 1073.0, 1150.0, 1073.0, 1183.0, 293.0, 1180.0], "score": 0.98, "text": " In Table 3 rows (B), we observe that reducing the attention key size"}, {"category_id": 15, "poly": [1106.0, 1148.0, 1402.0, 1150.0, 1402.0, 1183.0, 1106.0, 1180.0], "score": 0.97, "text": "hurts model quality. This"}, {"category_id": 15, "poly": [298.0, 1948.0, 1402.0, 1948.0, 1402.0, 1980.0, 298.0, 1980.0], "score": 0.99, "text": "Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful"}, {"category_id": 15, "poly": [298.0, 1980.0, 732.0, 1980.0, 732.0, 2010.0, 298.0, 2010.0], "score": 1.0, "text": "comments, corrections and inspiration."}, {"category_id": 15, "poly": [293.0, 190.0, 1404.0, 193.0, 1404.0, 231.0, 293.0, 229.0], "score": 0.99, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base"}, {"category_id": 15, "poly": [298.0, 227.0, 1400.0, 227.0, 1400.0, 259.0, 298.0, 259.0], "score": 1.0, "text": "model. All metrics are on the English-to-German translation development set, newstest2013. Listed"}, {"category_id": 15, "poly": [298.0, 259.0, 1404.0, 259.0, 1404.0, 291.0, 298.0, 291.0], "score": 0.99, "text": "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to"}, {"category_id": 15, "poly": [293.0, 291.0, 540.0, 289.0, 541.0, 321.0, 294.0, 323.0], "score": 1.0, "text": "per-wordperplexities."}, {"category_id": 15, "poly": [293.0, 190.0, 1404.0, 193.0, 1404.0, 231.0, 293.0, 229.0], "score": 0.99, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base"}, {"category_id": 15, "poly": [298.0, 227.0, 1400.0, 227.0, 1400.0, 259.0, 298.0, 259.0], "score": 1.0, "text": "model. All metrics are on the English-to-German translation development set, newstest2013. Listed"}, {"category_id": 15, "poly": [298.0, 259.0, 1404.0, 259.0, 1404.0, 291.0, 298.0, 291.0], "score": 0.99, "text": "perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to"}, {"category_id": 15, "poly": [293.0, 291.0, 540.0, 289.0, 541.0, 321.0, 294.0, 323.0], "score": 1.0, "text": "per-wordperplexities."}], "page_info": {"page_no": 8, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 1, "poly": [298.8049621582031, 245.8496856689453, 1410.182373046875, 245.8496856689453, 1410.182373046875, 2013.3106689453125, 298.8049621582031, 2013.3106689453125], "score": 0.9999617338180542}, {"category_id": 0, "poly": [296.43023681640625, 199.9274139404297, 460.6471862792969, 199.9274139404297, 460.6471862792969, 236.25718688964844, 296.43023681640625, 236.25718688964844], "score": 0.9999458193778992}, {"category_id": 2, "poly": [835.646240234375, 2061.894775390625, 867.4782104492188, 2061.894775390625, 867.4782104492188, 2085.936767578125, 835.646240234375, 2085.936767578125], "score": 0.9980964660644531}, {"category_id": 15, "poly": [307.0, 252.0, 1404.0, 254.0, 1404.0, 293.0, 307.0, 291.0], "score": 0.99, "text": "[1]  Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint"}, {"category_id": 15, "poly": [358.0, 284.0, 635.0, 284.0, 635.0, 316.0, 358.0, 316.0], "score": 1.0, "text": "arXiv:1607.06450, 2016"}, {"category_id": 15, "poly": [314.0, 341.0, 1402.0, 341.0, 1402.0, 374.0, 314.0, 374.0], "score": 0.99, "text": "[2]  Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly"}, {"category_id": 15, "poly": [356.0, 371.0, 1025.0, 367.0, 1026.0, 403.0, 356.0, 408.0], "score": 1.0, "text": "learning to align and translate. CoRR, abs/1409.0473, 2014."}, {"category_id": 15, "poly": [314.0, 426.0, 1402.0, 426.0, 1402.0, 458.0, 314.0, 458.0], "score": 0.98, "text": "[3]  Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural"}, {"category_id": 15, "poly": [358.0, 456.0, 1076.0, 454.0, 1076.0, 486.0, 358.0, 488.0], "score": 1.0, "text": "machine translation architectures. CoRR, abs/1703.03906, 2017."}, {"category_id": 15, "poly": [314.0, 511.0, 1402.0, 511.0, 1402.0, 543.0, 314.0, 543.0], "score": 0.98, "text": "[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine"}, {"category_id": 15, "poly": [360.0, 541.0, 903.0, 541.0, 903.0, 573.0, 360.0, 573.0], "score": 0.99, "text": "reading. arXiv preprint arXiv:1601.06733, 2016."}, {"category_id": 15, "poly": [312.0, 594.0, 1400.0, 591.0, 1400.0, 623.0, 312.0, 626.0], "score": 0.99, "text": "[5]  Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk."}, {"category_id": 15, "poly": [360.0, 626.0, 1402.0, 626.0, 1402.0, 658.0, 360.0, 658.0], "score": 1.0, "text": "and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical"}, {"category_id": 15, "poly": [358.0, 655.0, 910.0, 653.0, 910.0, 685.0, 358.0, 688.0], "score": 1.0, "text": "machine translation. CoRR, abs/1406.1078, 2014."}, {"category_id": 15, "poly": [310.0, 704.0, 1404.0, 706.0, 1404.0, 745.0, 309.0, 742.0], "score": 0.98, "text": "[6]  Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv"}, {"category_id": 15, "poly": [353.0, 738.0, 734.0, 731.0, 735.0, 770.0, 354.0, 777.0], "score": 1.0, "text": "preprint arXiv:1610.02357, 2016."}, {"category_id": 15, "poly": [314.0, 795.0, 1402.0, 795.0, 1402.0, 827.0, 314.0, 827.0], "score": 0.99, "text": "[7]  Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation"}, {"category_id": 15, "poly": [356.0, 823.0, 1333.0, 820.0, 1333.0, 859.0, 356.0, 862.0], "score": 0.99, "text": "of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014."}, {"category_id": 15, "poly": [314.0, 880.0, 1402.0, 880.0, 1402.0, 910.0, 314.0, 910.0], "score": 0.98, "text": "[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-"}, {"category_id": 15, "poly": [356.0, 905.0, 1254.0, 903.0, 1254.0, 942.0, 356.0, 944.0], "score": 1.0, "text": "tional sequence to sequence learning. arXiv preprint arXiv: 1705.03122v2, 2017."}, {"category_id": 15, "poly": [312.0, 960.0, 1404.0, 960.0, 1404.0, 999.0, 312.0, 999.0], "score": 0.96, "text": "[9]  Alex Graves.- Generating sequences with recurrent neural networks.  arXiv preprint"}, {"category_id": 15, "poly": [356.0, 992.0, 623.0, 990.0, 624.0, 1022.0, 356.0, 1024.0], "score": 1.0, "text": "arXiv:1308.0850, 2013."}, {"category_id": 15, "poly": [296.0, 1043.0, 1402.0, 1045.0, 1402.0, 1077.0, 296.0, 1075.0], "score": 0.98, "text": "[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-"}, {"category_id": 15, "poly": [358.0, 1077.0, 1400.0, 1077.0, 1400.0, 1109.0, 358.0, 1109.0], "score": 1.0, "text": "age recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern"}, {"category_id": 15, "poly": [353.0, 1105.0, 751.0, 1102.0, 751.0, 1141.0, 354.0, 1144.0], "score": 1.0, "text": "Recognition, pages 770-778, 2016."}, {"category_id": 15, "poly": [298.0, 1162.0, 1402.0, 1162.0, 1402.0, 1192.0, 298.0, 1192.0], "score": 0.99, "text": "[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jurgen Schmidhuber. Gradient fow in"}, {"category_id": 15, "poly": [356.0, 1187.0, 1148.0, 1189.0, 1148.0, 1228.0, 356.0, 1226.0], "score": 0.99, "text": "recurrent nets: the difficulty of learning long-term dependencies, 2001."}, {"category_id": 15, "poly": [298.0, 1247.0, 1404.0, 1247.0, 1404.0, 1279.0, 298.0, 1279.0], "score": 0.98, "text": "[12] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory.  Neural computation,"}, {"category_id": 15, "poly": [360.0, 1276.0, 614.0, 1276.0, 614.0, 1306.0, 360.0, 1306.0], "score": 1.0, "text": "9(8):1735-1780, 1997."}, {"category_id": 15, "poly": [296.0, 1327.0, 1402.0, 1329.0, 1402.0, 1361.0, 296.0, 1359.0], "score": 0.99, "text": "[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring"}, {"category_id": 15, "poly": [356.0, 1359.0, 1169.0, 1359.0, 1169.0, 1391.0, 356.0, 1391.0], "score": 0.99, "text": "the limits of language modeling. arXiv preprint arXiv: 1602.02410, 2016."}, {"category_id": 15, "poly": [298.0, 1414.0, 1402.0, 1414.0, 1402.0, 1446.0, 298.0, 1446.0], "score": 0.98, "text": "[14] Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference"}, {"category_id": 15, "poly": [360.0, 1446.0, 841.0, 1446.0, 841.0, 1476.0, 360.0, 1476.0], "score": 1.0, "text": "on Learning Representations (ICLR), 2016."}, {"category_id": 15, "poly": [300.0, 1499.0, 1402.0, 1499.0, 1402.0, 1531.0, 300.0, 1531.0], "score": 0.99, "text": "[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-"}, {"category_id": 15, "poly": [358.0, 1529.0, 1402.0, 1529.0, 1402.0, 1561.0, 358.0, 1561.0], "score": 1.0, "text": "ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv: 1610.10099v2,"}, {"category_id": 15, "poly": [360.0, 1561.0, 425.0, 1561.0, 425.0, 1586.0, 360.0, 1586.0], "score": 1.0, "text": "2017."}, {"category_id": 15, "poly": [300.0, 1613.0, 1402.0, 1613.0, 1402.0, 1645.0, 300.0, 1645.0], "score": 0.99, "text": "[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks."}, {"category_id": 15, "poly": [358.0, 1645.0, 1074.0, 1645.0, 1074.0, 1675.0, 358.0, 1675.0], "score": 0.99, "text": "In International Conference on Learning Representations, 2017."}, {"category_id": 15, "poly": [296.0, 1696.0, 1404.0, 1694.0, 1404.0, 1726.0, 296.0, 1728.0], "score": 0.99, "text": "[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015."}, {"category_id": 15, "poly": [298.0, 1751.0, 1402.0, 1751.0, 1402.0, 1783.0, 298.0, 1783.0], "score": 0.99, "text": "[18] Oleksi Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint"}, {"category_id": 15, "poly": [358.0, 1781.0, 640.0, 1781.0, 640.0, 1813.0, 358.0, 1813.0], "score": 1.0, "text": "arXiv:1703.10722, 2017."}, {"category_id": 15, "poly": [298.0, 1836.0, 1402.0, 1836.0, 1402.0, 1865.0, 298.0, 1865.0], "score": 0.99, "text": "[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen"}, {"category_id": 15, "poly": [360.0, 1865.0, 1402.0, 1865.0, 1402.0, 1898.0, 360.0, 1898.0], "score": 0.99, "text": "Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint"}, {"category_id": 15, "poly": [360.0, 1895.0, 642.0, 1895.0, 642.0, 1925.0, 360.0, 1925.0], "score": 1.0, "text": "arXiv:1703.03130, 2017."}, {"category_id": 15, "poly": [300.0, 1950.0, 1402.0, 1950.0, 1402.0, 1982.0, 300.0, 1982.0], "score": 0.99, "text": "[20] Samy Bengio Lukasz Kaiser. Can active memory replace attention? In Advances in Neural"}, {"category_id": 15, "poly": [358.0, 1982.0, 882.0, 1982.0, 882.0, 2012.0, 358.0, 2012.0], "score": 1.0, "text": "Information Processing Systems, (NIPS), 2016."}, {"category_id": 15, "poly": [294.0, 197.0, 460.0, 202.0, 459.0, 243.0, 293.0, 238.0], "score": 1.0, "text": "References"}, {"category_id": 15, "poly": [832.0, 2058.0, 871.0, 2058.0, 871.0, 2101.0, 832.0, 2101.0], "score": 1.0, "text": "10"}], "page_info": {"page_no": 9, "height": 2200, "width": 1700}}, {"layout_dets": [{"category_id": 1, "poly": [290.23052978515625, 184.09664916992188, 1414.702880859375, 184.09664916992188, 1414.702880859375, 1363.0975341796875, 290.23052978515625, 1363.0975341796875], "score": 0.9998990893363953}, {"category_id": 2, "poly": [834.680419921875, 2060.208740234375, 864.7958984375, 2060.208740234375, 864.7958984375, 2086.760986328125, 834.680419921875, 2086.760986328125], "score": 0.9995461702346802}, {"category_id": 15, "poly": [298.0, 206.0, 1402.0, 206.0, 1402.0, 238.0, 298.0, 238.0], "score": 0.98, "text": "[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-"}, {"category_id": 15, "poly": [358.0, 238.0, 1183.0, 238.0, 1183.0, 270.0, 358.0, 270.0], "score": 1.0, "text": "based neural machine translation. arXiv preprint arXiv:1508.04025, 2015."}, {"category_id": 15, "poly": [300.0, 289.0, 1402.0, 289.0, 1402.0, 321.0, 300.0, 321.0], "score": 0.99, "text": "[22] Ankur Parikh, Oscar Tackstrom, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention"}, {"category_id": 15, "poly": [358.0, 321.0, 1129.0, 321.0, 1129.0, 353.0, 358.0, 353.0], "score": 0.98, "text": "model. In Empirical Methods in Natural Language Processing, 2016."}, {"category_id": 15, "poly": [300.0, 371.0, 1402.0, 371.0, 1402.0, 403.0, 300.0, 403.0], "score": 0.99, "text": "[23]  Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive"}, {"category_id": 15, "poly": [356.0, 403.0, 986.0, 401.0, 986.0, 433.0, 356.0, 435.0], "score": 0.99, "text": "summarization. arXiv preprint arXiv:1705.04304, 2017."}, {"category_id": 15, "poly": [293.0, 449.0, 1404.0, 451.0, 1404.0, 490.0, 293.0, 488.0], "score": 0.98, "text": " [24] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv"}, {"category_id": 15, "poly": [351.0, 484.0, 737.0, 479.0, 737.0, 518.0, 351.0, 523.0], "score": 0.98, "text": " preprint arXiv: 1608.05859, 2016."}, {"category_id": 15, "poly": [298.0, 539.0, 1402.0, 539.0, 1402.0, 571.0, 298.0, 571.0], "score": 0.98, "text": "[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words"}, {"category_id": 15, "poly": [353.0, 564.0, 1035.0, 566.0, 1035.0, 605.0, 353.0, 603.0], "score": 0.99, "text": "with subword units. arXiv preprint arXiv: 1508.07909, 2015."}, {"category_id": 15, "poly": [300.0, 621.0, 1400.0, 621.0, 1400.0, 651.0, 300.0, 651.0], "score": 0.99, "text": "[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,"}, {"category_id": 15, "poly": [356.0, 646.0, 1404.0, 649.0, 1404.0, 688.0, 356.0, 685.0], "score": 0.99, "text": "and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts"}, {"category_id": 15, "poly": [360.0, 681.0, 878.0, 681.0, 878.0, 713.0, 360.0, 713.0], "score": 1.0, "text": "layer. arXiv preprint arXiv:1701.06538, 2017."}, {"category_id": 15, "poly": [300.0, 733.0, 1400.0, 733.0, 1400.0, 765.0, 300.0, 765.0], "score": 0.99, "text": "[27]  Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-"}, {"category_id": 15, "poly": [358.0, 765.0, 1402.0, 765.0, 1402.0, 798.0, 358.0, 798.0], "score": 0.98, "text": "nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine"}, {"category_id": 15, "poly": [358.0, 795.0, 855.0, 795.0, 855.0, 827.0, 358.0, 827.0], "score": 1.0, "text": "Learning Research, 15(1):1929-1958, 2014."}, {"category_id": 15, "poly": [300.0, 846.0, 1402.0, 846.0, 1402.0, 878.0, 300.0, 878.0], "score": 0.98, "text": "[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory"}, {"category_id": 15, "poly": [358.0, 878.0, 1400.0, 878.0, 1400.0, 910.0, 358.0, 910.0], "score": 0.99, "text": "networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,"}, {"category_id": 15, "poly": [356.0, 905.0, 1407.0, 905.0, 1407.0, 944.0, 356.0, 944.0], "score": 0.99, "text": "Advances in Neural Information Processing Systems 28, pages 2440-2448. Curran Associates,"}, {"category_id": 15, "poly": [356.0, 937.0, 480.0, 937.0, 480.0, 969.0, 356.0, 969.0], "score": 0.98, "text": "Inc., 2015."}, {"category_id": 15, "poly": [296.0, 988.0, 1404.0, 988.0, 1404.0, 1027.0, 296.0, 1027.0], "score": 0.98, "text": "[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural"}, {"category_id": 15, "poly": [356.0, 1018.0, 1374.0, 1015.0, 1374.0, 1054.0, 356.0, 1056.0], "score": 0.99, "text": "networks. In Advances in Neural Information Processing Systems, pages 3104-3112, 2014."}, {"category_id": 15, "poly": [298.0, 1075.0, 1404.0, 1075.0, 1404.0, 1105.0, 298.0, 1105.0], "score": 0.99, "text": "[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna."}, {"category_id": 15, "poly": [353.0, 1100.0, 1347.0, 1098.0, 1347.0, 1137.0, 353.0, 1139.0], "score": 0.99, "text": " Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015."}, {"category_id": 15, "poly": [293.0, 1148.0, 1404.0, 1153.0, 1404.0, 1192.0, 293.0, 1187.0], "score": 0.99, "text": "[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang"}, {"category_id": 15, "poly": [358.0, 1187.0, 1402.0, 1187.0, 1402.0, 1217.0, 358.0, 1217.0], "score": 1.0, "text": "Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google's neural machine"}, {"category_id": 15, "poly": [358.0, 1217.0, 1402.0, 1217.0, 1402.0, 1249.0, 358.0, 1249.0], "score": 0.99, "text": "translation system: Bridging the gap between human and machine translation. arXiv preprint"}, {"category_id": 15, "poly": [358.0, 1247.0, 638.0, 1247.0, 638.0, 1276.0, 358.0, 1276.0], "score": 0.99, "text": "arXiv:1609.08144, 2016."}, {"category_id": 15, "poly": [298.0, 1299.0, 1402.0, 1299.0, 1402.0, 1331.0, 298.0, 1331.0], "score": 0.99, "text": "[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with"}, {"category_id": 15, "poly": [356.0, 1329.0, 1323.0, 1327.0, 1324.0, 1359.0, 356.0, 1361.0], "score": 1.0, "text": "fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016."}, {"category_id": 15, "poly": [834.0, 2060.0, 866.0, 2060.0, 866.0, 2097.0, 834.0, 2097.0], "score": 1.0, "text": "11"}], "page_info": {"page_no": 10, "height": 2200, "width": 1700}}]